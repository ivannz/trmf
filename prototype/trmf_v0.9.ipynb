{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping the TRMF Regression algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook re-derives and implements the matrix factorization with\n",
    "autoregressive regularizer for time-series latent factors from\n",
    "> \n",
    "    .. [1] Yu, H. F., Rao, N., & Dhillon, I. S., (2016). \"Temporal\n",
    "           regularized matrix factorization for high-dimensional time\n",
    "           series prediction.\" In Advances in neural information processing\n",
    "           systems (pp. 847-855)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import numba as nb\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import some stuff copied from my library `SGIMC` for matrix completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trmf.tron import tron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(8945634)\n",
    "\n",
    "n_samples, n_components, n_targets, n_order = 120, 4, 16, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate coefficients of a stationary lag polynomial of order $p$. For explanation and derivations related to the synthetic dataset generator refer to `trmf_synthetic_example.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad = random_state.uniform(0, 1, size=(n_components, n_order))\n",
    "phi = random_state.uniform(0, 2, size=(n_components, n_order)) * np.pi\n",
    "\n",
    "phi /= 4  # confine the roots to the upper left quadrant\n",
    "\n",
    "roots = np.sqrt(rad) * (np.cos(phi) + np.sin(phi) * 1.j)\n",
    "\n",
    "n_cplx = max(0, n_order - 1) // 2\n",
    "n_real = n_order - n_cplx * 2\n",
    "\n",
    "roots = np.concatenate([\n",
    "    roots[:, :n_cplx], np.conj(roots[:, :n_cplx]), np.real(roots[:, -n_real:])\n",
    "], axis=-1)\n",
    "\n",
    "real_phi = np.stack([- np.poly(zeroes)[1:] for zeroes in roots], axis=0)\n",
    "real_ar_coef = real_phi[:, ::-1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the roots are within the unit circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([[c] * n_order for c in [\"C0\", \"C1\", \"C2\", \"C3\"]]).ravel()\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, projection=\"polar\")\n",
    "ax.scatter(np.angle(roots), np.abs(roots), c=colors, s=50)  #, c=colors, s=area, cmap='hsv', alpha=0.75)\n",
    "ax.set_rlim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate the autoregressive process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = random_state.normal(scale=0.1, size=(n_samples, n_components))\n",
    "for t in range(n_order, n_samples):\n",
    "#     noise[t] += np.einsum(\"il,li->i\", real_ar_coef, noise[t-n_order:t])\n",
    "    noise[t] = np.einsum(\"il,li->i\", real_ar_coef, noise[t-n_order:t])\n",
    "\n",
    "real_factors = noise.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_cols = 3\n",
    "n_rows = (n_components + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 1.5*n_rows),\n",
    "                         sharex=True, sharey=False)\n",
    "for j, ax in zip(range(n_components), axes.ravel()):\n",
    "    ax.plot(real_factors[:, j])\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate factor loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_loadings = random_state.uniform(-1, 2, size=(n_components, n_targets))\n",
    "real_loadings  = np.maximum(real_loadings, 0)\n",
    "\n",
    "noise = random_state.normal(scale=0.1, size=(n_samples, n_targets))\n",
    "\n",
    "targets = np.dot(real_factors, real_loadings) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_cols = 3\n",
    "n_rows = (n_targets + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 1.5*n_rows), sharex=True, sharey=False)\n",
    "for j, ax in zip(range(n_targets), axes.ravel()):\n",
    "    ax.plot(targets[:, j], lw=2)\n",
    "    ax.set_title(f\"\"\"target {j}\"\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(real_loadings, cmap=plt.cm.hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linalg.norm(real_loadings, ord=1, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization with time series structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a (directed) graph $G$ representing the **`is a component of`** binary relation\n",
    "on targets: for an y$u \\in G $ the fact that $v \\in G_u$ means that $u$ is required\n",
    "to produce $v$. The weighted adjacency matrix $A$ is given by $A_{uv} = \\sigma_{uv}\n",
    "1_{G_u}(v)$, where $\\sigma_{uv} = \\sigma(v\\to u)$ is the conversion\n",
    "rate from units of $v$ back to units of $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, $\\sigma(v\\to u)$ might represent minimial volume of input $u$ required\n",
    "to produce one unit of $v$ in the latter's production function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** that $\\lvert G_u \\rvert$ counts the number of neighbours of $u$ according the relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [TRMF](https://www.cs.utexas.edu/~rofuyu/papers/tr-mf-nips.pdf) model.\n",
    "Let's consider the following matrix decomposition for $Y$ -- $T\\times n$ matrix\n",
    "of multivariate time series:\n",
    "$$\n",
    "    \\min_{F, Z, \\phi}\n",
    "        \\tfrac1{2 T n} \\|Y - Z F \\|^2_2\n",
    "        + \\tfrac{\\lambda_F}2 \\mathcal{R}_F(F \\mid G)\n",
    "        + \\tfrac{\\lambda_Z}2 \\mathcal{R}_Z(Z \\mid\\, p, \\phi)\n",
    "        + \\tfrac{\\lambda_\\phi}2 \\mathcal{R}_\\phi(\\phi)\n",
    "\\,, $$\n",
    "where $Z$ is $T \\times d$ matrix of the time-series of $d$ latent factors and\n",
    "$F$ is the $d \\times n$ matrix of factor loadings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularizers are:\n",
    "\\begin{align}\n",
    "\\mathcal{R}_\\phi(\\phi)\n",
    "    &= \\tfrac1{d p} \\|\\phi\\|^2_F\n",
    "    \\,, \\\\\n",
    "\\mathcal{R}_F(F\\mid\\, G)\n",
    "    &= (1 - \\eta_{FG} - \\eta_{FN}) \\tfrac1{d n} \\|F\\|^2_F\n",
    "    + \\eta_{FN} \\overbrace{\\iota_{\\mathbb{R}_+^{d\\times n}}(F)}^{\\text{non-negativity}}\n",
    "    + \\eta_{FG} \\tfrac1{n} \\sum_{u\\in G}\n",
    "        \\overbrace{\n",
    "             \\tfrac1{d} \\Bigl\\|F e_u - \\tfrac1{\\lvert G_u\\rvert} \\sum_{v \\in G_u} F e_v A_{uv} \\Bigr\\|^2\n",
    "        }^{\\text{downstream tightness}}\n",
    "    \\,, \\\\\n",
    "\\mathcal{R}_Z(Z \\mid\\, p, \\phi)\n",
    "    &= (1 - \\eta_Z) \\tfrac1{T d} \\|Z\\|^2_F\n",
    "    + \\eta_Z \\tfrac1{d} \\sum_{j=1}^d\n",
    "        \\overbrace{\n",
    "            \\tfrac1{T-p} \\sum_{t=p+1}^T \\bigl( Z^j_t - \\sum_{i=1}^p \\phi_{ji} Z^j_{t-i} \\bigr)^2\n",
    "        }^{AR(p) \\, \\text{forcastible factors}}\n",
    "    \\,,\n",
    "\\end{align}\n",
    "where $\\phi$ is $d \\times p$ matrix of AR coefficients, related $\\eta$'s sum to $1$\n",
    "and are nonnegative and $Z^j_t = Z_{jt}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $A_{uv}$ is nonzero only if $v \\in G_u$, the `downstream tightness` regularizer\n",
    "simplifies to\n",
    "\\begin{equation}\n",
    "\\ldots\n",
    "    = \\sum_{u\\in G} \\bigl\\|F e_u - \\sum_{v \\in G}\n",
    "        F e_v e_v^{\\mathrm{T}} A^{\\mathrm{T}} e_u \\tfrac1{\\lvert G_u\\rvert} \\bigr\\|^2\n",
    "    = \\sum_{u\\in G} \\bigl\\|F e_u - F A^{\\mathrm{T}} D^{-1} e_u \\bigr\\|^2\n",
    "    = \\sum_{u\\in G} \\bigl\\|F (I - A^{\\mathrm{T}} D^{-1}) e_u\\bigr\\|^2\n",
    "    = \\bigl\\| F (I - A^{\\mathrm{T}} D^{-1}) \\bigr\\|^2_F\n",
    "    \\,,\n",
    "\\end{equation}\n",
    "where $D = \\mathop{\\mathrm{diag}}(\\lvert G_u \\rvert)_{u\\in G}$ is the out-degree matrix of $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible graph regularizer, which dominates the one above if $G$ is\n",
    "an undirected graph, is `pairwise tightness`\n",
    "\\begin{align}\n",
    "\\sum_{u \\in G}\\sum_{v\\in G_u} \\tfrac12 \\|F e_u - F e_v\\|^2\n",
    "    &= \\tfrac12 \\sum_{u,v \\in G} A_{uv} \\|F e_u - F e_v\\|^2\n",
    "    = \\tfrac12 \\sum_{u,v \\in G} A_{uv} \\|F e_u\\|^2\n",
    "        + A_{uv} \\|F e_v\\|^2\n",
    "        - 2 e_u^{\\mathrm{T}} F^{\\mathrm{T}} F e_v A_{uv}\n",
    "    \\\\\n",
    "    &= \\sum_{u \\in G} \\|F e_u\\|^2 \\sum_{v \\in G} \\tfrac12 (A_{uv} + A_{vu})\n",
    "        - \\sum_{u,v \\in G} e_v^{\\mathrm{T}} F^{\\mathrm{T}} F e_u A_{uv}\n",
    "    \\\\\n",
    "    &= \\mathop{\\mathrm{tr}} \\biggl(\n",
    "        F \\Bigl( \\sum_{u \\in G} e_u \\delta_u e_u^{\\mathrm{T}} \\Bigr) F^{\\mathrm{T}}\n",
    "        - F \\Bigl( \\sum_{u,v \\in G} e_u A_{uv} e_v^{\\mathrm{T}} \\Bigr) F^{\\mathrm{T}}\n",
    "    \\biggr)\n",
    "    = \\mathop{\\mathrm{tr}} \\bigl(\n",
    "        F \\underbrace{( D - A )}_{\\mathcal{L}} F^{\\mathrm{T}}\n",
    "    \\bigr)\n",
    "\\end{align}\n",
    "where $A$ is the symmetric weighted adjacency matrix of the graph $G$ (or\n",
    "a binary matrix if the edges of $G$ are unweighted) and $\\delta_u = \\sum_{v \\in G} A_{uv}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $AR(p)$-regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-derive theorem 1 from [[Yu, Rao, Dhillon; 2016]](https://www.cs.utexas.edu/~rofuyu/papers/tr-mf-nips.pdf),\n",
    "which reduces the AR regularizer to a graph-like regularizer. However, this time instead\n",
    "of the explicit lag structure of the AR, let's consider full AR$(p)$: no generality is lost,\n",
    "since we can impose the lag structure implicitly via force zeroeing in the AR polynomial\n",
    "coefficients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing $AR(p)$ for a single series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a series $z = (z_\\tau)_{\\tau=1}^T$ and AR$(p)$ coefficients $(\\beta_k)_{k=0}^p$,\n",
    "with $\\beta_0 = -1$. Sparse lag structure $L \\subseteq \\{1,\\,\\ldots,\\, p\\}$ is imposed by\n",
    "forcing $\\beta_k = 0$ for any $k \\notin L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AR$(p)$ residuals for series $z$ for given AR coefficients $\\beta$ is:\n",
    "\\begin{equation}\n",
    "r_t(z)\n",
    "    = \\sum_{k=0}^p \\beta_k z_{t-k}\n",
    "    = z_t - \\sum_{k=1}^p \\beta_k z_{t-k}\n",
    "    = z_t - \\beta_1 z_{t-1} - \\cdots - \\beta_p z_{t-p}\n",
    "    \\,,\n",
    "\\end{equation}\n",
    "for $t=p+1,\\,\\ldots,\\, T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** The $\\beta = \\phi_j$ is in little endian lag order: $\\beta_i$, $i=1..p$,\n",
    "corresponds to lag $p-i+1$. Also $z_{i:T+1-p+i}$ is the $p-i+1$ lagged slice of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(\"float64[:,::1](float64[:,::1], float64[:,::1])\",\n",
    "         fastmath=True, cache=False, error_model=\"numpy\")\n",
    "def ar_resid(Z, phi):\n",
    "    n_components, n_order = phi.shape\n",
    "\n",
    "    # compute the AR(p) residuals\n",
    "    resid = Z[n_order:].copy()\n",
    "    for k in range(n_order):\n",
    "        # r_t -= y_{t-(p-k)} * \\beta_{p - k} (phi is reversed beta)\n",
    "        resid -= Z[k:k - n_order] * phi[:, k]\n",
    "\n",
    "    return resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $AR(p)$ regualrizer  as a quadratic form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AR-regularizer can be represented as a quadratic form with a special matrix:\n",
    "\\begin{align}\n",
    "\\sum_{t=1+p}^T r_t^2(y)\n",
    "    &= \\sum_{t=1+p}^T \\bigl( \\sum_{k=0}^p \\beta_k y_{t-k} \\bigr)^2\n",
    "    = \\sum_{t=1+p}^T \\sum_{k=0}^p \\sum_{i=0}^p \\beta_k y_{t-k} \\beta_i y_{t-i}\n",
    "    = \\sum_{k=0}^p \\sum_{i=0}^p \\sum_{t=1+p-k}^{T-k} \\beta_k \\beta_i y_t y_{t-i+k}\n",
    "    \\\\\n",
    "    &= \\sum_{t=1}^T \\sum_{k=0}^p \\sum_{i=0}^p\n",
    "        \\beta_k \\beta_i y_t y_{t-i+k} 1_{\\{1+p\\leq t+k\\leq T\\}}\n",
    "    = \\sum_{t=1}^T \\sum_{s=1}^T \\sum_{k=0}^p \\sum_{i=0}^p\n",
    "        \\beta_k \\beta_i y_t y_s 1_{\\{1+p\\leq t+k\\leq T\\}} 1_{\\{s = t-i+k\\}}\n",
    "    \\\\\n",
    "    &= \\sum_{t=1}^T \\sum_{s=1}^T \\Bigl(\n",
    "        \\underbrace{\\sum_{k=0}^p \\sum_{i=0}^p\n",
    "            \\beta_k \\beta_i 1_{\\{1+p\\leq t+k\\leq T\\}} 1_{\\{s = t-i+k\\}}}_{A_{ts}}\n",
    "        \\Bigr) y_t y_s\n",
    "    = y^{\\mathrm{T}} A y\n",
    "    \\,.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $A_{ts}$ is given by\n",
    "\\begin{align}\n",
    "A_{ts}\n",
    "    &= \\sum_{k=0}^p \\sum_{i=0}^p\n",
    "        \\beta_k \\beta_i 1_{\\{1 + p \\leq t + k \\leq T\\}} 1_{\\{s + i = t + k\\}}\n",
    "    % i = k - m\n",
    "    % = \\sum_{k=0}^p \\sum_{m=k-p}^k\n",
    "    %     \\beta_k \\beta_{k - m} 1_{\\{1 + p \\leq t + k \\leq T\\}} 1_{\\{s + k - m = t + k\\}}\n",
    "    % = \\sum_{k=0}^p \\sum_{m=k-p}^k\n",
    "    %     \\beta_k \\beta_{k - m} 1_{\\{1 + p \\leq t + k \\leq T\\}} 1_{\\{s - t = m\\}}\n",
    "    = \\sum_{k=0}^p \\beta_k \\beta_{k - (s - t)}\n",
    "        1_{\\{1 + p \\leq t + k \\leq T\\}} 1_{\\{k - p \\leq s - t \\leq k\\}}\n",
    "    \\\\\n",
    "    % i = k + t\n",
    "    &= \\sum_{i=t}^{t+p} \\beta_{i - t} \\beta_{i - s}\n",
    "        1_{\\{1 + p \\leq i \\leq T\\}} 1_{\\{- p \\leq s - i \\leq 0\\}}\n",
    "    % = \\sum_{i=t}^{t + p} \\beta_{i - t} \\beta_{i - s}\n",
    "    %     1_{\\{1 + p \\leq i \\leq T\\}} 1_{\\{s \\leq i \\leq s + p\\}}\n",
    "    = \\sum_{i=t\\vee s}^{p+t\\wedge s} \\beta_{i - t} \\beta_{i - s}\n",
    "        1_{\\{1 + p \\leq i \\leq T\\}}\n",
    "    \\,.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the left-hand side of the regularizer term is nonnegative for any choice of coefficients\n",
    "$\\beta$ and series $y$, the resulting quadratic form is symmetric and positive semidefinite\n",
    "(indeed, $A_{ts} = A_{st}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $A$ is a $\\pm p$ banded matrix. Indeed, if $t > s + p$, then $p+t\\wedge s = p+s\\not \\geq t\\vee s = t$, and similarly for $s > t + p$.\n",
    "\n",
    "Let $\\lvert m \\rvert \\leq p$. Then for any $t,s=1,\\,\\ldots,\\,T$ with $s = t + m$\n",
    "(meaning that effectively $m \\leq p \\wedge (T-t)$) we have \n",
    "\\begin{equation}\n",
    "A_{t,t+m}\n",
    "    = \\sum_{k=0}^p \\beta_k \\beta_{k - m}\n",
    "        1_{\\{1 + p \\leq t + k \\leq T\\}} 1_{\\{k - p \\leq m \\leq k\\}}\n",
    "    % = \\sum_{i=t}^{t+p} \\beta_{i-t} \\beta_{i - t - m}\n",
    "    %     1_{\\{1 + p \\leq i \\leq T\\}} 1_{\\{i - t - p \\leq m \\leq i - t\\}}\n",
    "    = \\sum_{k=0}^p \\beta_k \\beta_{k - m}\n",
    "        1_{\\{1 + p \\leq t + k \\leq T\\}} 1_{\\{0 \\leq k - m \\leq p\\}}\n",
    "    = \\sum_{k=0\\vee m}^{p + 0 \\wedge m} \\beta_k \\beta_{k - m}\n",
    "        1_{\\{1 + p \\leq t + k \\leq T\\}}\n",
    "\\end{equation}\n",
    "Sine the matrix is symmetric, we may compute only $A_{t,t+m}$ for $m\\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the symmetric positive semi-definite matrix in the AR-regularizer (but this is **NEVER** needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(\"float64[:,::1](float64[::1], int64)\",\n",
    "         fastmath=True, cache=False, error_model=\"numpy\")\n",
    "def ar_single_matrix(beta, n_samples):\n",
    "    \"\"\"DEPRECATED this function is never used.\"\"\"\n",
    "    n_order = len(beta)\n",
    "    \n",
    "    # prepend a `-1` to beta\n",
    "    beta_ = np.full(1 + n_order, -1, dtype=beta.dtype)\n",
    "    beta_[1:] = beta[::-1]\n",
    "\n",
    "    # fill the matrix with zeros\n",
    "    ar_q = np.zeros((n_samples, n_samples), dtype=beta.dtype)\n",
    "    for t in range(n_samples):\n",
    "        # reuse the upper bound on `m`\n",
    "        m_max = min(1 + n_order, n_samples - t)\n",
    "        for m in range(m_max):\n",
    "            value = 0.\n",
    "            # 1 + p \\leq t + k < T + 1 if t = 1..T\n",
    "            #  <=> p \\leq t + k < T if t = 0:T\n",
    "            for k in range(max(m, n_order - t), m_max):\n",
    "                value += beta_[k] * beta_[k - m]\n",
    "            # end for\n",
    "            ar_q[t, t + m] = value\n",
    "            if m > 0:\n",
    "                ar_q[t + m, t] = value\n",
    "            # end if\n",
    "        # end for\n",
    "    # end for\n",
    "    return ar_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the gradient and hessian-vector products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to take derivatives of the AR regularizer w.r.t. $y$:\n",
    "\\begin{equation}\n",
    "\\tfrac12 \\frac{\\partial}{\\partial y} \\sum_{t=1+p}^T r_t^2(y)\n",
    "    = \\tfrac12 \\frac{\\partial}{\\partial y} \\sum_{t=1+p}^T \\bigl( y_t - \\sum_{k=1}^p \\beta_k y_{t-k} \\bigr)^2\n",
    "    = \\tfrac12 \\frac{\\partial}{\\partial y} y^{\\mathrm{T}} A y\n",
    "    = A y\n",
    "    \\,.\n",
    "\\end{equation}\n",
    "In fact the hessian-vector product of the AR-regularizer $\\mathrm{Hv}(z; y)$ is $A z$, which\n",
    "means that the expression for the gradient and the hessian-vector product can be implemented\n",
    "in one function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easier to derive the gradient of the sum-of-squared residuals using its\n",
    "representation as a quadratic form:\n",
    "\\begin{align}\n",
    "    e_t^{\\mathrm{T}} A z\n",
    "        &= \\sum_{s=1}^T A_{ts} z_s\n",
    "        = \\sum_{s=1}^T \\bigl(\n",
    "            \\sum_{i=t\\vee s}^{p+t\\wedge s} \\beta_{i - t} \\beta_{i - s}\n",
    "                1_{\\{1 + p \\leq i \\leq T\\}}\n",
    "            \\bigr) z_s\n",
    "        = \\sum_{s=1}^T\n",
    "            \\sum_{i=1+p}^T \\beta_{i - t} \\beta_{i - s} z_s\n",
    "                1_{\\{s \\leq i \\leq s+p\\}}\n",
    "                1_{\\{t \\leq i \\leq t+p\\}}\n",
    "        \\\\\n",
    "        &= \\sum_{i=1+p}^T\n",
    "            \\beta_{i - t}\n",
    "            1_{\\{t \\leq i \\leq t+p\\}}\n",
    "            \\sum_{s=1}^T \\beta_{i - s} z_s\n",
    "                1_{\\{0 \\leq i - s \\leq p\\}}\n",
    "        = \\sum_{i=1+p}^T\n",
    "            \\beta_{i - t}\n",
    "            1_{\\{t \\leq i \\leq t+p\\}}\n",
    "            \\sum_{k=i-T}^{i-1} \\beta_k z_{i-k}\n",
    "                1_{\\{0 \\leq k \\leq p\\}}\n",
    "        \\\\\n",
    "        &= \\sum_{i=1+p}^T\n",
    "            \\beta_{i - t}\n",
    "            1_{\\{0 \\leq i - t \\leq p\\}}\n",
    "            \\sum_{k=0}^p \\beta_k z_{i-k}\n",
    "        = \\sum_{j=1+p-t}^{T-t}\n",
    "            \\beta_j\n",
    "            1_{\\{0 \\leq j \\leq p\\}}\n",
    "            r_{t+j}(z)\n",
    "        = \\sum_{j=0}^p \\beta_j r_{t+j}(z)\n",
    "            1_{\\{1+p \\leq t+j \\leq T\\}}\n",
    "    \\,.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence:\n",
    "\\begin{equation}\n",
    "A z\n",
    "    = \\sum_{t=1}^T e_t \\Bigl(\n",
    "        \\sum_{i=0}^p \\beta_i r_{t+i}(z) 1_{\\{1+p \\leq t+i \\leq T\\}}\n",
    "    \\Bigr)\n",
    "    = \\sum_{i=0}^p \\beta_i\n",
    "        \\Bigl( \\sum_{t=1}^T e_t r_{t+i}(z) 1_{\\{1+p \\leq t+i \\leq T\\}} \\Bigr)\n",
    "    = \\sum_{i=0}^p \\beta_i\n",
    "        \\Bigl( \\sum_{t=1+p}^T e_{t-i} e_t^{\\mathrm{T}} \\Bigr) r(z)\n",
    "    \\,,\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below computes the hessian-vector product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(\"float64[:,::1](float64[:,::1], float64[:,::1], float64[:,::1])\",\n",
    "         fastmath=True, cache=False, error_model=\"numpy\")\n",
    "def ar_hess_vect(V, Z, phi):\n",
    "    n_components, n_order = phi.shape\n",
    "\n",
    "    # compute the AR(p) residuals over V\n",
    "    resid = ar_resid(V, phi)\n",
    "\n",
    "    # get the derivative w.r.t. the series\n",
    "    hess_v = np.zeros_like(V)\n",
    "    for k in range(n_order):\n",
    "        hess_v[k:k - n_order] += resid * phi[:, k]\n",
    "    hess_v[n_order:] -= resid\n",
    "\n",
    "    return hess_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the derivative of the AR regularizer w.r.t. all $y_t$ is:\n",
    "\\begin{equation}\n",
    "\\Bigl( \\frac{\\partial}{\\partial y_t}\n",
    "    \\tfrac12 \\sum_{s=1+p}^T \\bigl( y_s - \\sum_{k=1}^p \\beta_k y_{s-k} \\bigr)^2\n",
    "\\Bigr)_{t=1}^T\n",
    "    = A y\n",
    "    = \\sum_{t=1}^T e_t \\Bigl(\n",
    "        \\sum_{i=0}^p \\beta_i r_{t+i}(y) 1_{\\{1+p \\leq t+i \\leq T\\}}\n",
    "    \\Bigr)\n",
    "    \\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(\"float64[:,::1](float64[:,::1], float64[:,::1])\",\n",
    "         fastmath=True, cache=False, error_model=\"numpy\")\n",
    "def ar_grad(Z, phi):\n",
    "    return ar_hess_vect(Z, Z, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed graph based regularzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neighbourhood variance regularizer is given by\n",
    "\\begin{equation}\n",
    "\\sum_{u\\in G} \\bigl\\|F e_u - \\sum_{v \\in G}\n",
    "        F e_v A_{uv} \\tfrac1{\\lvert G_u\\rvert} \\bigr\\|^2\n",
    "    = \\sum_{u\\in G} \\bigl\\|F e_u - \\sum_{v \\in G}\n",
    "        F e_v e_v^{\\mathrm{T}} A^{\\mathrm{T}} e_u \\tfrac1{\\lvert G_u\\rvert} \\bigr\\|^2\n",
    "    = \\bigl\\| F (I - A^{\\mathrm{T}} D^{-1}) \\bigr\\|^2_F\n",
    "    \\,,\n",
    "\\end{equation}\n",
    "where $D = \\mathop{\\mathrm{diag}}(\\lvert G_u \\rvert)_{u\\in G}$ is the out-degree matrix of $G$\n",
    "and $A$ is the weighted adjacency matrix of the graph $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_resid(F, adj):\n",
    "    # `adj` is the weighted adjacency matrix: A_{ij} = \\sigma_{ij} 1_{G}(i\\to j)\n",
    "    # get the downstream average: right mutliply by a transpose of CSR (mem CSC) is cheap\n",
    "    out = safe_sparse_dot(F, adj.T, dense_output=True)\n",
    "\n",
    "    # get the outgoing degree: |j \\in G_i| = |i \\to u for any u|\n",
    "    deg = adj.getnnz(axis=1)[np.newaxis]\n",
    "\n",
    "    # out_sum is zero if there are no neighbours\n",
    "    mask = deg > 0\n",
    "    np.divide(out, deg, where=mask, out=out)\n",
    "    return np.subtract(F, out, where=mask, out=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full differential of the regularier is:\n",
    "\\begin{equation}\n",
    "\\tfrac12 \\partial \\bigl\\| F (I - A^{\\mathrm{T}} D^{-1}) \\bigr\\|^2_F\n",
    "    = \\mathop{\\mathrm{tr}} \\bigl(\n",
    "        F (I - A^{\\mathrm{T}} D^{-1})\n",
    "            (I - A^{\\mathrm{T}} D^{-1})^{\\mathrm{T}}\n",
    "            (\\partial F)^{\\mathrm{T}}\n",
    "    \\bigr)\n",
    "    = \\mathop{\\mathrm{tr}} \\bigl(\n",
    "        R(F) (I - D^{-1} A) (\\partial F)^{\\mathrm{T}}\n",
    "    \\bigr)\n",
    "    = \\mathop{\\mathrm{tr}} \\bigl(\n",
    "         (\\partial F)^{\\mathrm{T}} R(F) (I - D^{-1} A)\n",
    "    \\bigr)\n",
    "    \\,,\n",
    "\\end{equation}\n",
    "where $R(F)$ is the residual from subtracting the mean across the neighboring nodes from the\n",
    "node's vector, i.e. $R(F) = F - F A^{\\mathrm{T}} D^{-1}$.\n",
    "\n",
    "Therefore, the gradient is $\\tfrac12 \\nabla_F \\bigl\\| F (I - A^{\\mathrm{T}} D^{-1}) \\bigr\\|^2_F$\n",
    "is $R(F) (I - D^{-1} A)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_grad(F, adj):\n",
    "    resid = graph_resid(F, adj)\n",
    "    deg = np.maximum(adj.getnnz(axis=1), 1)[np.newaxis]\n",
    "    return resid - safe_sparse_dot(resid / deg, adj, dense_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the hessian-vector product is derived from:\n",
    "$$\n",
    "\\tfrac12 \\langle \\nabla_{FF} \\bigl\\| F (I - A^{\\mathrm{T}} D^{-1}) \\bigr\\|^2_F, V\\rangle\n",
    "    = \\partial \\bigl(\n",
    "        R(F) (I - A^{\\mathrm{T}} D^{-1})^{\\mathrm{T}}\n",
    "    \\bigr) \\Big\\vert_{\\partial F = V}\n",
    "    = R(V) (I - D^{-1} A)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_hess_vect(V, F, adj):\n",
    "    return graph_grad(V, adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy neg-log-det-cov regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the log-det regularizer in the latent factors:\n",
    "$$\n",
    "    \\mathcal{R}(Z)\n",
    "        = - \\log \\lvert Z^{\\mathrm{T}} Z \\rvert\n",
    "    \\,. $$\n",
    "**NOTE** that due to $Z^{\\mathrm{T}} Z$ term this regularizer is non-convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neglogdetsym_value(Z, nugget=1e-3):\n",
    "    ZTZ = np.dot(Z.T, Z)\n",
    "    ZTZ[::len(ZTZ) + 1] += nugget\n",
    "    return - np.log(np.linalg.det(ZTZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the [cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) that\n",
    "* $ \\partial \\Sigma^{-1} = - \\Sigma^{-1} (\\partial \\Sigma) \\Sigma^{-1}$\n",
    "* $\\partial \\det \\Sigma = (\\det \\Sigma) \\mathop{\\mathrm{tr}} (\\Sigma^{-1} \\partial \\Sigma)$\n",
    "* $\\partial Z^{\\mathrm{T}} Z = (\\partial Z)^{\\mathrm{T}} Z + Z^{\\mathrm{T}} \\partial Z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's full differential is\n",
    "\\begin{align}\n",
    "\\partial \\mathcal{R}(Z)\n",
    "    &= - \\mathop{\\mathrm{tr}} (Z^{\\mathrm{T}} Z)^{-1} ( \\partial Z^{\\mathrm{T}} Z + Z^{\\mathrm{T}}\\partial Z)\n",
    "    % = \\mathop{\\mathrm{tr}} (Z^{\\mathrm{T}} Z)^{-1} (\\partial Z)^{\\mathrm{T}} Z\n",
    "    % + \\mathop{\\mathrm{tr}} (Z^{\\mathrm{T}} Z)^{-1} Z^{\\mathrm{T}} \\partial Z\n",
    "    \\\\\n",
    "    &= - \\sum_{ij} \\mathop{\\mathrm{tr}} e_j \\partial_{ij} e_i^{\\mathrm{T}} Z (Z^{\\mathrm{T}} Z)^{-1}\n",
    "        + \\mathop{\\mathrm{tr}} (Z^{\\mathrm{T}} Z)^{-1} Z^{\\mathrm{T}} (e_i \\partial_{ij} e_j^{\\mathrm{T}})\n",
    "    % = \\partial_{ij} e_i^{\\mathrm{T}} Z (Z^{\\mathrm{T}} Z)^{-1} e_j\n",
    "    % + e_j^{\\mathrm{T}} (Z^{\\mathrm{T}} Z)^{-1} Z^{\\mathrm{T}} e_i \\partial_{ij}\n",
    "    \\\\\n",
    "    &= - 2 \\sum_{ij} \\partial_{ij} e_i^{\\mathrm{T}} Z (Z^{\\mathrm{T}} Z)^{-1} e_j\n",
    "    \\,.\n",
    "\\end{align}\n",
    "\n",
    "Therefore, $\\nabla \\mathcal{R}(Z) = - 2 Z (Z^{\\mathrm{T}} Z)^{-1}$ -- $-2$ times\n",
    "the transposed Moore-Penrose inverse of $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neglogdetsym_grad(Z, nugget=1e-12):\n",
    "    return - 2 * np.linalg.pinv(Z, nugget).T\n",
    "    # u, s, vh = np.linalg.svd(Z, full_matrices=False)\n",
    "    # u *= s[np.newaxis] / (s[np.newaxis]**2 + nugget)\n",
    "    # 2 * np.dot(u, vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.linalg.pinv` computes something like this:\n",
    "if $U \\Sigma V^{\\mathrm{T}}$ is the SVD of $Z$ ($V$ is square), then\n",
    "$$ Z (Z^{\\mathrm{T}}Z)^{-1}\n",
    "    = U \\Sigma V^{\\mathrm{T}} \\bigl(\n",
    "        V \\Sigma^{\\mathrm{T}} U^{\\mathrm{T}}\n",
    "        U \\Sigma V^{\\mathrm{T}}\n",
    "    \\bigr)^{-1}\n",
    "    = U \\Sigma V^{\\mathrm{T}} \\bigl(\n",
    "        V \\Sigma^{2} V^{\\mathrm{T}}\n",
    "    \\bigr)^{-1}\n",
    "    = U \\Sigma V^{\\mathrm{T}} V^{-\\mathrm{T}} \\Sigma^{-2} V^{-1}\n",
    "    = U \\Sigma^{-1} V^{\\mathrm{T}}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consder the following matrix differentials, each of which are a linear forms w.r.t $\\partial Z$:\n",
    "\\begin{align}\n",
    "\\partial (Z^{\\mathrm{T}} Z)^{-1}\n",
    "    &= - (Z^{\\mathrm{T}} Z)^{-1} \\bigl( \\partial (Z^{\\mathrm{T}} Z) \\bigr) (Z^{\\mathrm{T}} Z)^{-1}\n",
    "    = - (Z^{\\mathrm{T}} Z)^{-1} \\bigl(\n",
    "        (\\partial Z)^{\\mathrm{T}} Z + Z^{\\mathrm{T}} (\\partial Z)\n",
    "    \\bigr) (Z^{\\mathrm{T}} Z)^{-1}\n",
    "    \\\\\n",
    "\\partial \\bigl( Z (Z^{\\mathrm{T}} Z)^{-1} \\bigr)\n",
    "    &= (\\partial Z) (Z^{\\mathrm{T}} Z)^{-1} + Z \\partial (Z^{\\mathrm{T}} Z)^{-1}\n",
    "    = (\\partial Z) (Z^{\\mathrm{T}} Z)^{-1}\n",
    "    - Z (Z^{\\mathrm{T}} Z)^{-1} \\bigl(\n",
    "        (\\partial Z)^{\\mathrm{T}} Z + Z^{\\mathrm{T}} (\\partial Z)\n",
    "    \\bigr) (Z^{\\mathrm{T}} Z)^{-1}\n",
    "    \\,,\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus if we want to compute $\\langle \\nabla^2 \\mathcal{R}(Z), V \\rangle$ we may simply\n",
    "do $\\partial \\bigl(\\nabla \\mathcal{R}(Z)\\bigr) \\big\\vert_{\\partial Z = V}$ -- get the full\n",
    "differential at $Z$ and apply it to $V$:\n",
    "\\begin{align}\n",
    "- \\tfrac12 \\langle \\nabla^2 \\mathcal{R}(Z), V \\rangle\n",
    "    &= V (Z^{\\mathrm{T}} Z)^{-1}\n",
    "    - Z \\Bigl(\n",
    "        \\underbrace{(Z^{\\mathrm{T}} Z)^{-1} V^{\\mathrm{T}} Z (Z^{\\mathrm{T}} Z)^{-1}}_{W^{\\mathrm{T}}}\n",
    "        + \\underbrace{(Z^{\\mathrm{T}} Z)^{-1} Z^{\\mathrm{T}} V (Z^{\\mathrm{T}} Z)^{-1}}_{W}\n",
    "    \\Bigr)\n",
    "    \\\\\n",
    "    &= \\bigl\\{ V (Z^{\\mathrm{T}} Z)^{-1} - Z (W^{\\mathrm{T}} + W) \\bigr\\}\n",
    "    \\,.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the hessian-vector product based on one SVD call and the\n",
    "handling of ill-conditioned matrices as in `numpy.linalg.pinv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neglogdetsym_hess_vect(V, Z, nugget=1e-6):\n",
    "    # pinvV = np.dot(np.linalg.pinv(Z, nugget), V)\n",
    "    u, s, vh = np.linalg.svd(Z, full_matrices=False)\n",
    "\n",
    "    # do the same cutoff as in np.linalg.pinv(...)\n",
    "    large = s > nugget * np.max(s, axis=-1, keepdims=True)\n",
    "    s = np.divide(1, s, where=large, out=s)\n",
    "    s[~large] = 0\n",
    "\n",
    "    vh *= s[..., np.newaxis]\n",
    "#     ZinvZ = np.dot(vh.T, vh)\n",
    "#     pinvV = np.dot(np.dot(vh.T, np.dot(u.T, V)), ZinvZ)\n",
    "    W = np.einsum(\"ba,cb,cd,ed,ef->af\", vh, u, V, vh, vh)\n",
    "    VZ = np.einsum(\"ab,cb,cd->ad\", V, vh, vh)\n",
    "    return -2 * (VZ - np.dot(Z, W + W.T))\n",
    "    \n",
    "#     assert np.allclose(pinvV2, pinvV)\n",
    "\n",
    "    return -2 * (np.dot(V, ZinvZ) - np.dot(Z, pinvV + pinvV.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup common variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_Z, C_F, C_phi = 1e-1, 1e0, 1e-1\n",
    "eta_Z, eta_F, adj = 0.05, 0.0, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, Z, F, phi = targets, real_factors, real_loadings, real_ar_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_targets = Y.shape\n",
    "n_components, n_order = phi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a numerical algorithm for trmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the explicit form the matrix factorisation problem is\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{F \\geq 0, Z, \\phi}{\\text{minimize}}\n",
    "  & & \\tfrac1{2 T n} \\|Y - Z F\\|^2\n",
    "      + \\tfrac{\\lambda_F}2 \\Bigl(\n",
    "            (1 - \\eta_F) \\tfrac1{d n} \\|F\\|^2\n",
    "            + \\eta_F \\tfrac1{d n} \\bigl\\| F - F A^{\\mathrm{T}} D^{-1} \\bigr\\|^2\n",
    "        \\Bigr)\n",
    "      \\\\\n",
    "& & & + \\tfrac{\\lambda_Z}2 \\Bigl(\n",
    "            (1 - \\eta_Z) \\tfrac1{T d} \\|Z\\|^2\n",
    "            + \\eta_Z \\tfrac1{(T - p) d}\n",
    "            \\sum_{j=1}^d \\sum_{t=p+1}^T \\bigl(\n",
    "                Z_{tj} - \\sum_{i=1}^p \\phi_{ji} Z_{t-i,j}\n",
    "            \\bigr)^2\n",
    "        \\Bigr)\n",
    "      + \\tfrac{\\lambda_\\phi}2\n",
    "        \\tfrac1{d p} \\|\\phi \\|^2\n",
    "      \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REWRITE**\n",
    "Notice that when $Z$ is fixed the problem w.r.t $F$ and $\\phi$ is jointly convex. Therefore we may use the following optimization streategy:\n",
    "* minimize jonitly over $(F, \\phi)$ given some estimate of $Z$\n",
    "* find the optimal latent series $Z$ for the fixed $(F, \\phi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the scaled objective (by $\\times Tn$) and the constituent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objective_components(Y, Z, F, phi, C_Z, C_F, C_phi, eta_Z, eta_F, adj):\n",
    "    # get the shapes\n",
    "    n_samples, n_targets = Y.shape\n",
    "    n_components, n_order = phi.shape\n",
    "\n",
    "    # reg: Z\n",
    "    reg_z_l2 = (np.linalg.norm(Z, ord=\"fro\") ** 2) / (n_samples * n_components)\n",
    "    if (n_samples > n_order) and (eta_Z > 0):\n",
    "        reg_z_ar_j = np.linalg.norm(ar_resid(Z, phi), ord=2, axis=0) ** 2\n",
    "        reg_z_ar = np.sum(reg_z_ar_j) / ((n_samples - n_order) * n_components)\n",
    "    else:\n",
    "        reg_z_ar, eta_Z = 0., 0.\n",
    "    # end if\n",
    "\n",
    "    reg_z = reg_z_l2 * (1 - eta_Z) + reg_z_ar * eta_Z\n",
    "\n",
    "    # reg: F\n",
    "    reg_f_l2 = (np.linalg.norm(F, ord=\"fro\") ** 2) / (n_components * n_targets)\n",
    "    if sp.issparse(adj) and (eta_F > 0):\n",
    "        resid = graph_resid(F, adj)\n",
    "        reg_f_graph = (np.linalg.norm(resid, ord=\"fro\") ** 2) / (n_components * n_targets)\n",
    "    else:\n",
    "        reg_f_graph, eta_F = 0., 0.\n",
    "    # end if\n",
    "\n",
    "    reg_f = reg_f_l2 * (1 - eta_F) + reg_f_graph * eta_F\n",
    "\n",
    "    # reg: phi\n",
    "    reg_phi_l2 = (np.linalg.norm(phi, ord=\"fro\") ** 2) / (n_components * n_order)\n",
    "    \n",
    "    reg_phi = reg_phi_l2\n",
    "\n",
    "    # the reconstruction loss term\n",
    "    loss = np.linalg.norm(Y - np.dot(Z, F), ord=\"fro\") ** 2\n",
    "\n",
    "    regularizers = C_Z * reg_z + C_F * reg_f + C_phi * reg_phi\n",
    "    objective = 0.5 * (loss + regularizers * n_samples * n_targets)\n",
    "\n",
    "    return {\n",
    "        \"objective\": objective,\n",
    "        \"loss\": loss / (n_samples * n_targets),\n",
    "        \"reg\": regularizers,\n",
    "        \"reg_z\": reg_z,\n",
    "        \"reg_z_l2\": reg_z_l2,\n",
    "        \"reg_z_ar\": reg_z_ar,\n",
    "        \"reg_f\": reg_f,\n",
    "        \"reg_f_l2\": reg_f_l2,\n",
    "        \"reg_f_graph\": reg_f_graph,\n",
    "        \"reg_phi\": reg_phi,\n",
    "        \"reg_phi_l2\": reg_phi_l2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compo = get_objective_components(Y, Z, F, phi, C_Z, C_F, C_phi, eta_Z, eta_F, adj)\n",
    "compo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $F$-subproblem: no non-negativity constraint\n",
    "\n",
    "Let's ignore the non-negativity constraint for a while\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{F\\,\\mid\\,Z, \\phi}{\\text{minimize}}\n",
    "  & & J(F)\n",
    "      = \\tfrac1{2 T n} \\|Y - Z F\\|^2\n",
    "      + \\tfrac{\\lambda_F}2 \\Bigl(\n",
    "          (1 - \\eta_F) \\tfrac1{d n} \\|F\\|^2\n",
    "          + \\eta_F \\tfrac1{d n} \\bigl\\| F - F A^{\\mathrm{T}} D^{-1} \\bigr\\|^2\n",
    "      \\Bigr)\n",
    "     \\,,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $A$ is the adjacency matrix of the directed relation graph on the set of items $1,\\,\\ldots,\\,n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled value of the $F$-step objective: $(T n) J(F)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_step_tron_valj(f, Y, Z, C_F, eta_F, adj):\n",
    "    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n",
    "    F = f.reshape(n_components, n_targets)\n",
    "\n",
    "    objective = np.linalg.norm(Y - np.dot(Z, F), ord=\"fro\") ** 2\n",
    "    if C_F > 0:\n",
    "        reg_f_l2 = np.linalg.norm(F, ord=\"fro\") ** 2\n",
    "\n",
    "        if sp.issparse(adj) and (eta_F > 0):\n",
    "            reg_f_graph = np.linalg.norm(graph_resid(F, adj), ord=\"fro\") ** 2\n",
    "        else:\n",
    "            reg_f_graph, eta_F = 0., 0.\n",
    "        # end if\n",
    "\n",
    "        reg_f = reg_f_l2 * (1 - eta_F) + reg_f_graph * eta_F\n",
    "        objective += reg_f * (C_F * n_samples / n_components)\n",
    "    # end if\n",
    "\n",
    "    return 0.5 * objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(f_step_tron_valj(F.ravel(), Y, Z, C_F, eta_F, adj),\n",
    "                  (compo[\"loss\"] + (compo[\"reg_f_l2\"] * (1 - eta_F) + compo[\"reg_f_graph\"] * eta_F) * C_F) * 0.5 * n_samples * n_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the loss w.r.t. $F$ is given by\n",
    "\\begin{align}\n",
    "(T n) \\nabla J\n",
    "    &= - Z^{\\mathrm{T}} (Y - Z F) + (T n) \\lambda_F \\bigl(\n",
    "        (1 - \\eta_F) \\tfrac1{d n} F\n",
    "        + \\eta_F \\tfrac1{d n} F (I - A^{\\mathrm{T}} D^{-1}) (I - D^{-1} A)\n",
    "    \\bigr)\n",
    "    \\\\\n",
    "    &= - Z^{\\mathrm{T}} Y\n",
    "    + \\bigl( Z^{\\mathrm{T}} Z + (1 - \\eta_F) \\tfrac{T \\lambda_F }{d} \\bigr) F\n",
    "    + \\eta_F \\tfrac{T \\lambda_F }{d} R(F) (I - D^{-1} A)\n",
    "    % \\\\\n",
    "    % &= - Z^{\\mathrm{T}} Y\n",
    "    % + \\bigl( Z^{\\mathrm{T}} Z + \\tfrac{\\lambda_F T}{d} I \\bigr) F\n",
    "    % - \\eta_F \\tfrac{\\lambda_F T}{d} R(F) (I - D^{-1} A)\n",
    "    \\,.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_step_tron_grad(f, Y, Z, C_F, eta_F, adj):\n",
    "    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n",
    "    coef = C_F * n_samples / n_components\n",
    "\n",
    "    F = f.reshape(n_components, n_targets)\n",
    "\n",
    "    ZTY, ZTZ = np.dot(Z.T, Y), np.dot(Z.T, Z)\n",
    "    if (C_F > 0) and (eta_F < 1):\n",
    "        ZTZ.flat[::n_components + 1] += (1 - eta_F) * coef\n",
    "\n",
    "    grad = np.dot(ZTZ, F) - ZTY\n",
    "    if (C_F > 0) and sp.issparse(adj) and (eta_F > 0):\n",
    "        grad += graph_grad(F, adj) * eta_F * coef\n",
    "\n",
    "    return grad.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full differential of the gradient of $J$ is\n",
    "\\begin{equation}\n",
    "(T n) \\partial \\nabla J\n",
    "    = \\bigl( X^{\\mathrm{T}} X + (1 - \\eta_F) \\tfrac{T \\lambda_F }{d} \\bigr) \\partial F\n",
    "    + \\eta_F \\tfrac{T \\lambda_F }{d} \\bigl( \\partial F (I - A^{\\mathrm{T}} D^{-1}) \\bigr) (I - D^{-1} A)\n",
    "    \\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hessian vector-product is given by:\n",
    "$$\n",
    "\\langle (T n) \\nabla_{FF} J, Z \\rangle\n",
    "    = (T n) \\partial \\nabla J \\Big\\vert_{\\partial F = Z}\\\n",
    "    = \\bigl( X^{\\mathrm{T}} X + (1 - \\eta_F) \\tfrac{T \\lambda_F }{d} \\bigr) Z\n",
    "    + \\eta_F \\tfrac{T \\lambda_F }{d} R(Z) (I - D^{-1} A)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_step_tron_hess(v, Y, Z, C_F, eta_F, adj):\n",
    "    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n",
    "    coef = C_F * n_samples / n_components\n",
    "\n",
    "    V = v.reshape(n_components, n_targets)\n",
    "\n",
    "    ZTZ = np.dot(Z.T, Z)\n",
    "    if (C_F > 0) and (eta_F < 1):\n",
    "        ZTZ.flat[::n_components + 1] += (1 - eta_F) * coef\n",
    "\n",
    "    hess_v = np.dot(ZTZ, V)\n",
    "    if (C_F > 0) and sp.issparse(adj) and (eta_F > 0):\n",
    "        # should call graph_hess_vect(Z, F, adj)\n",
    "        hess_v += graph_grad(V, adj) * eta_F * coef\n",
    "\n",
    "    return hess_v.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the unconstrained $F$-step using Trust Region Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_step_tron(F, Y, Z, C_F, eta_F, adj, rtol=5e-2, atol=1e-4, verbose=False, **kwargs):\n",
    "    f_call = f_step_tron_valj, f_step_tron_grad, f_step_tron_hess\n",
    "\n",
    "    tron(f_call, F.ravel(), n_iterations=5, rtol=rtol, atol=atol,\n",
    "         args=(Y, Z, C_F, eta_F, adj), verbose=verbose)\n",
    "\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $F$-subproblem: fast prox gradient with non-negativity constraint\n",
    "\n",
    "The $F$ subproblem with non-negativity constraints has is much harder:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{F\\,\\mid\\,X, \\phi}{\\text{minimize}}\n",
    "  & & J(F) + \\iota_{\\mathbb{R}^{d\\times n}_+}\n",
    "     \\,,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $J(F)$ is the objective from the unsonstrained problem, $\\iota_C(x) = (+\\infty)[x \\notin C]$\n",
    "is the convex indicator of the convex set $C$ and $\\mathbb{R}^{d\\times n}_+$ corresponds\n",
    "to the set of $d\\times n$ matrices with non-negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_step_prox_func(F, Y, Z, C_F, eta_F, adj):\n",
    "    return f_step_tron_valj(F.ravel(), Y, Z, C_F, eta_F, adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the general problem\n",
    "$$\n",
    "    \\min_x f(x) + g(x)\n",
    "    \\,, $$\n",
    "where $g$ is convex proper lsc function and $f$ is smooth with Lipschitz gradient.\n",
    "The following are equivalent to iff $x$ is the solution.\n",
    "\\begin{align}\n",
    "    & \\tfrac1\\nu x \\in \\nabla f(x) + \\tfrac1\\nu x + \\partial g(x)\n",
    "    \\\\\n",
    "    & x - \\nu \\nabla f(x) \\in (\\mathrm{id} + \\nu \\partial g)(x)\n",
    "    \\\\\n",
    "    & x = (\\mathrm{id} + \\nu \\partial g)^{-1}\\bigl(x - \\nu \\nabla f(x) \\bigr)\n",
    "    = {\\text{prox}}_{\\nu \\iota_{\\mathbb{R}^d_+}}\\bigl(x - \\nu \\nabla f(x) \\bigr)\n",
    "    \\,.\n",
    "\\end{align}\n",
    "Therefore if we find a fixed point of $x \\mapsto {\\text{prox}}_{\\nu g}\\bigl(x - \\nu \\nabla f(x) \\bigr)$\n",
    "then we have found the optimum.\n",
    "\n",
    "Check [this](http://web1.sph.emory.edu/users/hwu30/teaching/statcomp/Notes/lecture2-optimization.pdf),\n",
    "[this](https://web.stanford.edu/class/ee364b/lectures/monotone_slides.pdf) and\n",
    "[this](https://www.stats.ox.ac.uk/~lienart/blog_opti_pgd.html) out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, since $J$ is smoooth, the optimization problem is equivalent to finding\n",
    "a fixed point of the well known forward-backward splitting operator, which gives the\n",
    "proximal gradient descent step:\n",
    "$$ F_{t+1}\n",
    "    = {\\text{prox}}_{\\eta \\iota_{\\mathbb{R}^d_+}}\n",
    "        \\Bigl(F_t - \\nabla_F J(F_t) \\nu \\Bigr)\n",
    "    \\,, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fast proximal gradient method the gradient we previously computed is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_step_prox_grad(F, Y, Z, C_F, eta_F, adj):\n",
    "    return f_step_tron_grad(F.ravel(), Y, Z, C_F, eta_F, adj).reshape(F.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the prox-operator of the projection on the nonnegative \"orthant\":\n",
    "$$\n",
    "{\\text{prox}}_{\\nu \\iota_{\\mathbb{R}^d_+}}(x)\n",
    "    = \\arg \\min_{z\\geq 0} \\tfrac1{2 \\nu} \\| z - x\\|^2\n",
    "\\,. $$\n",
    "Since the constraints and the objective are separable, solving the optimization\n",
    "problem directly for each element of $x$ yields\n",
    "$$\n",
    "{\\text{prox}}_{\\nu \\iota_{\\mathbb{R}^d_+}}(x)\n",
    "    = (\\max\\{x_i, 0\\})_{i=1}^n\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_step_prox(F, Y, Z, C_F, eta_F, adj, lip=1e-2, n_iter=25, alpha=1.0, **kwargs):\n",
    "    gamma_u, gamma_d = 2, 1.1\n",
    "\n",
    "    # get the gradient\n",
    "    grad = f_step_prox_grad(F, Y, Z, C_F, eta_F, adj)\n",
    "    grad_F = np.dot(grad.flat, F.flat)\n",
    "\n",
    "    f0, lip0 = f_step_prox_func(F, Y, Z, C_F, eta_F, adj), lip\n",
    "    for _ in range(n_iter):\n",
    "        # F_new = (1 -  alpha) * F + alpha * np.maximum(F - lr * grad, 0.)\n",
    "        # prox-sgd operation\n",
    "        F_new = np.maximum(F - grad / lip, 0.)\n",
    "\n",
    "        # fgm lipschitz search\n",
    "        delta = f_step_prox_func(F_new, Y, Z, C_F, eta_F, adj) - f0\n",
    "        linear = np.dot(grad.flat, F_new.flat) - grad_F\n",
    "        quad = np.linalg.norm(F_new - F, ord=\"fro\") ** 2\n",
    "        if delta <= linear + lip * quad / 2:\n",
    "            break\n",
    "        lip *= gamma_u\n",
    "    # end for\n",
    "#     lip = max(lip0, lip / gamma_d)\n",
    "    lip = lip / gamma_d\n",
    "\n",
    "    return F_new, lip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $F$-subproblem: no non-negativity constraint and no graph regularizer\n",
    "\n",
    "This corresponds to the case of $\\eta_F = 0$ and relaxed non-negativity constraints.\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{F\\,\\mid\\,X, \\phi}{\\text{minimize}}\n",
    "  & & J(F)\n",
    "      = \\tfrac1{2 T n} \\|Y - Z F\\|^2\n",
    "      + \\tfrac{\\lambda_F}2 \\tfrac1{d n} \\|F\\|^2\n",
    "     \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The first-order-conditions are $0 = (T n)\\nabla J(F)$, which yields the ridge regression-like\n",
    "closed form solution\n",
    "$$\n",
    "    F = \\bigl(Z^{\\mathrm{T}} Z + \\tfrac{\\lambda_F T}{d} I \\bigl)^{-1} Z^{\\mathrm{T}} Y\n",
    "    = V \\bigl(\\Sigma^2 + \\tfrac{\\lambda_F T}{d} I \\bigl)^{-1} \\Sigma U^{\\mathrm{T}} Y\n",
    "    \\,, $$\n",
    "for the SVD $Z = U \\Sigma V^{\\mathrm{T}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_step_ridge(F, Y, Z, C_F, eta_F, adj, **kwargs):\n",
    "    assert np.isclose(eta_F, 0) and (C_F > 0)\n",
    "\n",
    "    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n",
    "\n",
    "    u, s, vh = np.linalg.svd(Z, full_matrices=False)\n",
    "    vh *= s[..., np.newaxis]\n",
    "    vh /= s[..., np.newaxis]**2 + C_F * n_samples / n_components\n",
    "\n",
    "    return np.dot(vh.T, safe_sparse_dot(u.T, Y, dense_output=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniform $F$-step dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_step(F, Y, Z, C_F, eta_F, adj, kind=\"fgm\", **kwargs):\n",
    "    lip = np.inf\n",
    "    if kind == \"fgm\":\n",
    "        F, lip = f_step_prox(F, Y, Z, C_F, eta_F, adj, **kwargs)\n",
    "    elif kind == \"ridge\":\n",
    "        F = f_step_ridge(F, Y, Z, C_F, eta_F, adj, **kwargs)\n",
    "    elif kind == \"tron\":\n",
    "        F = f_step_tron(F, Y, Z, C_F, eta_F, adj, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"\"\"Unrecognozed optiomization `{kind}`\"\"\")\n",
    "    return F, lip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\phi$-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the explicit form the matrix factorisation problem is\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\phi\\,\\mid\\,F, Z}{\\text{minimize}}\n",
    "  & & Q(\\phi)\n",
    "      = \\tfrac{\\lambda_\\phi}2 \\tfrac1{d p} \\|\\phi \\|^2\n",
    "      + \\tfrac{\\lambda_Z}2 \\eta_Z \\tfrac1{(T - p) d} \\sum_{j=1}^d \\sum_{t=p+1}^T \\bigl(\n",
    "          Z_{tj} - \\sum_{i=1}^p \\phi_{ji} Z_{t-i,j}\n",
    "      \\bigr)^2\n",
    "      \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda_\\phi = 0$ then $\\phi$ is the OLS solution of the vector autoregression\n",
    "with diagonal coefficient matrix\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\phi\\,\\mid\\,F, Z}{\\text{minimize}}\n",
    "  & & \\sum_{j=1}^d \\sum_{t=p+1}^T \\bigl(\n",
    "          Z_{tj} - \\sum_{i=1}^p \\phi_{ji} Z_{t-i,j}\n",
    "      \\bigr)^2\n",
    "      \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\lambda_X = 0$ or $\\eta_X = 0$ then $\\phi = 0$, since it solves\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\phi\\,\\mid\\,F, Z}{\\text{minimize}}\n",
    "  & & Q(\\phi)\n",
    "      = \\tfrac{\\lambda_\\phi}2 \\tfrac1{d p} \\|\\phi \\|^2\n",
    "      \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So consider the case when $\\lambda_\\phi, \\lambda_Z, \\eta_Z > 0$. Here the problem\n",
    "is equivalent to\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\phi\\,\\mid\\,F, Z}{\\text{minimize}}\n",
    "  & & Q(\\phi)\n",
    "      = \\tfrac{\\lambda_\\phi (T - p)}{2 \\lambda_Z \\eta_Z p} \\|\\phi \\|^2\n",
    "      + \\tfrac12 \\sum_{j=1}^d \\sum_{t=p+1}^T \\bigl(\n",
    "          Z_{tj} - \\sum_{i=1}^p \\phi_{ji} Z_{t-i,j}\n",
    "      \\bigr)^2\n",
    "      \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subproblem for $\\phi$ separates in $d$ ridge-autoregression problems:\n",
    "for each $j=1,\\,\\ldots,\\,d$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    & \\underset{\\beta}{\\text{minimize}}\n",
    "      & & \\tfrac12\n",
    "              \\sum_{t=p+1}^T \\bigl(\n",
    "                  y_t - \\sum_{i=1}^p \\beta_k y_{t-i}\n",
    "              \\bigr)^2\n",
    "          + \\tfrac{\\lambda_\\phi (T - p)}{2 \\lambda_Z \\eta_Z p} \\|\\beta\\|^2\n",
    "          \\,,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $\\beta = \\phi_j$ and $y = Z e_j$ -- the time series of the $j$-th factor.\n",
    "\n",
    "Using the time series autoregressive embedding of order $p$ this problem\n",
    "becomes the familiar ridge regresssion:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    & \\underset{\\beta}{\\text{minimize}}\n",
    "      & & \\tfrac12 \\|y_{p+1:} - X \\beta \\|^2\n",
    "          + \\tfrac{\\lambda_\\phi (T - p)}{2 \\lambda_Z \\eta_Z p} \\|\\beta \\|^2\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $X_t = (y_{t-(p-k)})_{k=0}^{p-1} \\in \\mathbb{R}^{p\\times 1}$ and $X \\in \\mathbb{R}^{(T-p) \\times p}$.\n",
    "The final $X$ looks like $(y_{s:T-p+s})_{s=1}^p$, where $y_{s:t} = (y_\\tau)_{s \\leq \\tau < t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is\n",
    "$$\n",
    "\\beta\n",
    "    = \\bigl(\n",
    "        X^{\\mathrm{T}}X + \\tfrac{\\lambda_\\phi (T - p)}{\\lambda_Z \\eta_Z p} I\n",
    "    \\bigr)^{-1} X^{\\mathrm{T}} y_{p+1:}\n",
    "    \\,. $$\n",
    "Let $U \\Sigma V^{\\mathrm{T}}$ be the thin-SVD decomposition of $X$ (assumed to be full rank). Then\n",
    "$$\n",
    "\\beta\n",
    "    = \\bigl(\n",
    "        V \\Sigma^2 V^{\\mathrm{T}} + \\tfrac{\\lambda_\\phi (T - p)}{\\lambda_Z \\eta_Z p} I\n",
    "    \\bigr)^{-1} V \\Sigma U^{\\mathrm{T}} y_{p+1:}\n",
    "    = V \\bigl(\n",
    "        \\Sigma^2 +  \\tfrac{\\lambda_\\phi (T - p)}{\\lambda_Z \\eta_Z p} I\n",
    "    \\bigr)^{-1} \\Sigma U^{\\mathrm{T}} y_{p+1:}\n",
    "    = \\sum_{s=1}^d V e_s\n",
    "        \\frac{\\lambda_Z \\eta_Z p \\sigma_s}{\\lambda_Z \\eta_Z p \\sigma_s^2 + \\lambda_\\phi (T - p)}\n",
    "        (U e_s)^{\\mathrm{T}} y_{p+1:}\n",
    "    \\,. $$\n",
    "**Remember** right action affects columns, left -- rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "def phi_step(phi, Z, C_Z, C_phi, eta_Z, nugget=1e-8):\n",
    "    # return a set of independent AR(p) ridge estimates.\n",
    "    (n_components, n_order), n_samples = phi.shape, Z.shape[0]\n",
    "    if n_order < 1 or n_components < 1:\n",
    "        return np.empty((n_components, n_order))\n",
    "\n",
    "    if not ((C_Z > 0) and (eta_Z > 0)):\n",
    "        return np.zeros_like(phi)\n",
    "\n",
    "    # embed into the last dimensions\n",
    "    shape = Z.shape[1:] + (Z.shape[0] - n_order, n_order + 1)\n",
    "    strides = Z.strides[1:] + Z.strides[:1] + Z.strides[:1]\n",
    "    Z_view = as_strided(Z, shape=shape, strides=strides)\n",
    "\n",
    "    # split into y (d x T-p) and Z (d x T-p x p) (all are views!)\n",
    "    y, Z_lagged = Z_view[..., -1], Z_view[..., :-1]\n",
    "\n",
    "    # compute the SVD: thin, but V is d x p x p\n",
    "    U, s, Vh = np.linalg.svd(Z_lagged, full_matrices=False)\n",
    "    if C_phi > 0:\n",
    "        # the {V^{H}}^{H} (\\Sigma^2 + C I)^{-1} \\Sigma part is reduced\n",
    "        #  to columnwise operations\n",
    "        gain = C_Z * eta_Z * n_order * s\n",
    "        gain /= gain * s + C_phi * (n_samples - n_order)\n",
    "    else:\n",
    "        # do the same cutoff as in np.linalg.pinv(...)\n",
    "        large = s > nugget * np.max(s, axis=-1, keepdims=True)\n",
    "        gain = np.divide(1, s, where=large, out=s)\n",
    "        gain[~large] = 0\n",
    "    # end if\n",
    "\n",
    "    # get the U' y part and the final estimate\n",
    "    # $\\phi_j$ corresponds to $p-j$-th lag $j = 0,\\,\\ldots,\\,p-1$\n",
    "    return np.einsum(\"ijk,ij,isj,is->ik\", Vh, gain, U, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some sanity unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# C_Z, C_phi, eta_Z = 1e-1, 1e-1, 1.0\n",
    "# n_samples, n_order, n_components = 1000, 4, 25\n",
    "\n",
    "# X = np.random.normal(size=(n_samples, n_components))\n",
    "# phi = np.zeros((n_components, n_order))\n",
    "\n",
    "C = C_phi * (n_samples - n_order) / (C_Z * eta_Z * n_order)\n",
    "\n",
    "models = []\n",
    "for j in range(Z.shape[1]):\n",
    "    ZZ = np.stack([\n",
    "        Z[l:l-n_order, j] for l in range(n_order)\n",
    "    ], axis=1)\n",
    "    yy = Z[n_order:, [j]]\n",
    "    models.append(Ridge(alpha=C, fit_intercept=False, solver=\"svd\").fit(ZZ, yy))\n",
    "\n",
    "assert np.allclose(phi_step(phi, Z, C_Z, C_phi, eta_Z),\n",
    "                   np.concatenate([mdl.coef_ for mdl in models], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the estimates against the real coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_hat = phi_step(phi, Z, C_Z*0+1, C_phi*0, eta_Z*0+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(phi, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(phi_hat, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the $AR(p)$ coefficients aren't precisely estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $Z$-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent autoregressive factors $Z$ are estimated conditional on $F$ and $\\phi$\n",
    "by solving the following problem:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{Z\\,\\mid\\, F, \\phi}{\\text{minimize}}\n",
    "  & & H(Z)\n",
    "      = \\tfrac1{2 T n} \\|Y - Z F\\|^2\n",
    "      + \\tfrac{\\lambda_Z}2 \\Bigl(\n",
    "            (1 - \\eta_Z) \\tfrac1{T d} \\|Z\\|^2\n",
    "            + \\eta_Z \\tfrac1{(T - p) d}\n",
    "            \\sum_{j=1}^d \\sum_{t=p+1}^T \\bigl(\n",
    "                Z_{tj} - \\sum_{i=1}^p \\phi_{ji} Z_{t-i,j}\n",
    "            \\bigr)^2\n",
    "        \\Bigr)\n",
    "      \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_step_tron_valh(z, Y, F, phi, C_Z, eta_Z):\n",
    "    n_samples, n_targets = Y.shape\n",
    "    n_components, n_order = phi.shape\n",
    "    Z = z.reshape(n_samples, n_components)\n",
    "\n",
    "    objective = np.linalg.norm(Y - np.dot(Z, F), ord=\"fro\") ** 2\n",
    "    if C_Z > 0:\n",
    "        reg_z_l2 = (np.linalg.norm(Z, ord=\"fro\") ** 2)\n",
    "        if (n_samples > n_order) and (eta_Z > 0):\n",
    "            reg_z_ar_j = np.linalg.norm(ar_resid(Z, phi), ord=2, axis=0) ** 2\n",
    "            reg_z_ar = np.sum(reg_z_ar_j) * n_samples / (n_samples - n_order)\n",
    "        else:\n",
    "            reg_z_ar, eta_Z = 0., 0.\n",
    "        # end if\n",
    "\n",
    "        reg_z = reg_z_l2 * (1 - eta_Z) + reg_z_ar * eta_Z\n",
    "        objective += C_Z * n_targets / n_components * reg_z\n",
    "    # end if\n",
    "\n",
    "    return 0.5 * objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the implementation is sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(x_step_tron_valh(Z.ravel(), Y, F, phi, C_Z, eta_Z),\n",
    "                  (compo[\"loss\"] + (compo[\"reg_z_l2\"] * (1 - eta_Z) + compo[\"reg_z_ar\"] * eta_Z) * C_Z) * 0.5 * n_samples * n_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the scaled full objective w.r.t $Z$ is\n",
    "\\begin{equation}\n",
    "(T n) \\nabla H(Z)\n",
    "    = - Y F^{\\mathrm{T}} + Z F F^{\\mathrm{T}}\n",
    "    + (1 - \\eta_Z) \\tfrac{\\lambda_Z n}{d} Z\n",
    "    + \\eta_Z \\tfrac{\\lambda_Z n}{d} \\tfrac{T}{T - p}\n",
    "        \\sum_{j=1}^d A(\\phi_j) Z e_j e_j^{\\mathrm{T}}\n",
    "    \\equiv - \\mathop{\\text{vecc}}((Y - Z F) F^{\\mathrm{T}})\n",
    "    + \\mathcal{A}(\\phi) \\mathop{\\text{vecc}}(Z)\n",
    "    + \\eta \\mathop{\\text{vecc}}(Z)\n",
    "    \\,,\n",
    "\\end{equation}\n",
    "where $\\mathop{\\text{vecc}}$ is the column-major stacked representation of $Z$\n",
    "($\\mathop{\\text{vec}}$ is the standard row-major stacked representation of $Z$) and\n",
    "\\begin{equation}\n",
    "\\mathcal{A}(\\phi)\n",
    "    = \\begin{pmatrix}\n",
    "        \\lambda_1 A(\\phi_1) & \\cdots & 0                     \\\\\n",
    "        \\vdots                & \\ddots & \\vdots                \\\\\n",
    "        0                     & \\cdots & \\lambda_d A(\\phi_d) \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "* $\\mathop{\\text{vecc}}(M) = \\mathop{\\text{vec}}(M^{\\mathrm{T}})$, $\\mathop{\\text{vec}}(ABC) = \\bigl(A \\otimes C^{\\mathrm{T}}\\bigr)\\mathop{\\text{vec}}(B)$, and $\\mathop{\\text{vecc}}(ABC) = \\bigl(C^{\\mathrm{T}} \\otimes A\\bigr)\\mathop{\\text{vecc}}(B)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_step_tron_grad(z, Y, F, phi, C_Z, eta_Z):\n",
    "    n_samples, n_targets = Y.shape\n",
    "    n_components, n_order = phi.shape\n",
    "    coef = C_Z * n_targets / n_components\n",
    "\n",
    "    Z = z.reshape(n_samples, n_components)\n",
    "\n",
    "    YFT, FFT = np.dot(Y, F.T), np.dot(F, F.T)\n",
    "    if (C_Z > 0) and (eta_Z < 1):\n",
    "        FFT.flat[::n_components + 1] += (1 - eta_Z) * coef\n",
    "\n",
    "    grad = np.dot(Z, FFT) - YFT\n",
    "    if (C_Z > 0) and (eta_Z > 0):\n",
    "        ratio = n_samples / (n_samples - n_order)\n",
    "        grad += ar_grad(Z, phi) * ratio * eta_Z * coef\n",
    "\n",
    "    return grad.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the scaled full objective w.r.t $Z$ is\n",
    "\\begin{equation}\n",
    "\\langle \\nabla_{ZZ} (T n) H(Z), V \\rangle\n",
    "    = \\partial (T n) \\nabla H(Z) \\big\\vert_{\\partial Z = V}\n",
    "    = V \\bigl( F F^{\\mathrm{T}} + (1 - \\eta_Z) \\tfrac{\\lambda_Z n}{d} I \\bigr)\n",
    "    + \\eta_Z \\tfrac{\\lambda_Z n}{d} \\tfrac{T}{T - p}\n",
    "        \\sum_{j=1}^d A(\\phi_j) V e_j e_j^{\\mathrm{T}}\n",
    "    \\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_step_tron_hess(v, Y, F, phi, C_Z, eta_Z):\n",
    "    n_samples, n_targets = Y.shape\n",
    "    n_components, n_order = phi.shape\n",
    "    coef = C_Z * n_targets / n_components\n",
    "\n",
    "    V = v.reshape(n_samples, n_components)\n",
    "\n",
    "    FFT = np.dot(F, F.T)\n",
    "    if (C_Z > 0) and (eta_Z < 1):\n",
    "        FFT.flat[::n_components + 1] += (1 - eta_Z) * coef\n",
    "\n",
    "    hess_v = np.dot(V, FFT)\n",
    "    if (C_Z > 0) and (eta_Z > 0):\n",
    "        # should call ar_hess_vect(V, Z, adj) but no Z is available\n",
    "        ratio = n_samples / (n_samples - n_order)\n",
    "        hess_v += ar_grad(V, phi) * ratio * eta_Z * coef\n",
    "\n",
    "    return hess_v.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the $X$-step by TRON (modified code from `liblinear`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_step_tron(Z, Y, F, phi, C_Z, eta_Z, rtol=5e-2, atol=1e-4, verbose=False):\n",
    "    f_call = x_step_tron_valh, x_step_tron_grad, x_step_tron_hess\n",
    "\n",
    "    tron(f_call, Z.ravel(), n_iterations=5, rtol=rtol, atol=atol,\n",
    "         args=(Y, F, phi, C_Z, eta_Z), verbose=verbose)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete trmf procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SVD to initialize the factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def trmf_init(data, n_components, n_order, random_state=None):\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    n_samples, n_targets = data.shape\n",
    "    if sp.issparse(data):\n",
    "        U, s, Vh = sp.linalg.svds(data)\n",
    "    else:\n",
    "        U, s, Vh = np.linalg.svd(data, full_matrices=False)\n",
    "\n",
    "    factors = U[:, :n_components].copy()\n",
    "    loadings = Vh[:n_components].copy()\n",
    "    loadings *= s[:n_components, np.newaxis]\n",
    "\n",
    "    n_svd_factors = factors.shape[1]\n",
    "    if n_svd_factors < n_components:\n",
    "        random_factors = random_state.normal(\n",
    "            scale=0.01, size=(n_samples, n_components - n_svd_factors))\n",
    "        factors = np.concatenate([factors, random_factors], axis=1)\n",
    "\n",
    "    n_svd_loadings = loadings.shape[0]\n",
    "    if n_svd_loadings < n_components:\n",
    "        random_loadings = random_state.normal(\n",
    "            scale=0.01, size=(n_components - n_svd_loadings, n_targets))\n",
    "        loadings = np.concatenate([loadings, random_loadings], axis=0)\n",
    "\n",
    "    phi = np.zeros((n_components, n_order))\n",
    "    ar_coef = phi_step(phi, factors, 1.0, 0., 1.0)\n",
    "    return factors, loadings, ar_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"Finished\" procedure for running the time series regularized matrix factorization.\n",
    "\n",
    "* This has **no** intercept and exogenous regressor support!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trmf(data, n_components, n_order, C_Z, C_F, C_phi, eta_Z, eta_F=0., adj=None,\n",
    "         tol=1e-6, n_max_iterations=1000, n_max_mf_iter=5, f_step_kind=\"fgm\",\n",
    "         verbose=False, random_state=None):\n",
    "\n",
    "    if not (adj is None or sp.issparse(adj)):\n",
    "        raise TypeError(\"\"\"The adjacency matrix must be sparse.\"\"\")\n",
    "\n",
    "    if not all(C >= 0 for C in (C_Z, C_F, C_phi)):\n",
    "        raise ValueError(\"\"\"Negative ridge regularizer coefficient.\"\"\")\n",
    "\n",
    "    if not all(0 <= eta <= 1 for eta in (eta_Z, eta_F)):\n",
    "        raise ValueError(\"\"\"Share `eta` is not within `[0, 1]`.\"\"\")\n",
    "\n",
    "    if not (n_components > 0):\n",
    "        raise ValueError(\"\"\"Empty latent factors are not supported.\"\"\")\n",
    "\n",
    "    # prepare smart guesses\n",
    "    factors, loadings, ar_coef = trmf_init(data, n_components, n_order,\n",
    "                                           random_state=random_state)\n",
    "\n",
    "    # initialize the outer loop\n",
    "    ZF, lip = np.dot(factors, loadings), 500.0\n",
    "    ZF_old_norm, delta = np.linalg.norm(ZF, ord=\"fro\"), +np.inf\n",
    "    for iteration in range(n_max_iterations):\n",
    "        if verbose and (iteration % 5 == 0):\n",
    "            compo = get_objective_components(data, factors, loadings, ar_coef,\n",
    "                                             C_Z, C_F, C_phi, eta_Z, eta_F, adj)\n",
    "            print(\"\"\"iter {:03d} obj {objective:5.3e} loss {loss:5.3e} \"\"\"\\\n",
    "                  \"\"\"Z {reg_z:4.2e} F {reg_f:4.2e} A {reg_phi:4.2e} \"\"\"\\\n",
    "                  \"\"\"lip {lip:3.1e}\"\"\".format(iteration, **compo, lip=lip))\n",
    "\n",
    "        if (delta <= ZF_old_norm * tol) and (iteration > 0):\n",
    "            break\n",
    "\n",
    "        # update (F, Z), then phi\n",
    "        for inner_iter in range(n_max_mf_iter):\n",
    "            loadings, lip = f_step(loadings, data, factors, C_F, eta_F, adj,\n",
    "                                   kind=f_step_kind, lip=lip)\n",
    "\n",
    "            factors = x_step_tron(factors, data, loadings, ar_coef, C_Z, eta_Z)\n",
    "        ar_coef = phi_step(ar_coef, factors, C_Z, C_phi, eta_Z)\n",
    "\n",
    "        # recompute the reconstruction and convergence criteria\n",
    "        ZF, ZF_old = np.dot(factors, loadings), ZF\n",
    "        delta = np.linalg.norm(ZF - ZF_old, ord=\"fro\")\n",
    "        ZF_old_norm = np.linalg.norm(ZF_old, ord=\"fro\")\n",
    "    # end for\n",
    "\n",
    "    return factors, loadings, ar_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AR(p) dynamic forecast\n",
    "$$\n",
    "    \\hat{y}_{t+h\\mid t}\n",
    "        = \\phi_1 \\hat{y}_{t+h-1\\mid t} + \\cdots\n",
    "        + \\phi_p \\hat{y}_{t+h-p\\mid t}\n",
    "        = \\sum_{k=1}^p \\phi_k \\hat{y}_{t+h-k\\mid t}\n",
    "    \\,, $$\n",
    "where $\\hat{y}_{t+h-k\\mid t} = y_{t+h-k}$ if $k \\geq h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMEBER** $\\phi = (\\phi_i)_{i=0}^{p-1}$ runs\n",
    "in the reverse order of lags from $p$ up to $1$, i.e. $\\phi_i$ corresponds to\n",
    "$L^{p-i}$, lag $p-i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trmf_forecast_factors(n_ahead, ar_coef, prehist):\n",
    "    n_components, n_order = ar_coef.shape\n",
    "    if n_ahead < 1:\n",
    "        raise ValueError(\"\"\"`n_ahead` must be a positive integer.\"\"\")\n",
    "\n",
    "    if len(prehist) < n_order:\n",
    "        raise TypeError(\"\"\"Factor history is too short.\"\"\")\n",
    "\n",
    "    forecast = np.concatenate([\n",
    "        prehist[-n_order:] if n_order > 0 else prehist[:0],\n",
    "        np.zeros((n_ahead, n_components))\n",
    "    ], axis=0)\n",
    "\n",
    "    # compute the dynamic forecast\n",
    "    for h in range(n_order, n_order + n_ahead):\n",
    "        # ar_coef are stored in little endian lag order: from lag p to lag 1\n",
    "        #  from the least recent to the most recent!\n",
    "        forecast[h] = np.einsum(\"il,li->i\", ar_coef, forecast[h - n_order:h])\n",
    "\n",
    "    return forecast[-n_ahead:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the algorithm on synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorizing the synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the synthetic data into train and test periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_targets, test_targets = train_test_split(targets, test_size=.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never forget to centre and scale the train dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scl = StandardScaler(with_mean=True, with_std=True)\n",
    "YY = scl.fit_transform(train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a model with more factors but the same AR$(p)$ latent process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_order = 8\n",
    "n_components = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_Z, C_F, C_phi = 5e-2, 5e-1, 1e-6\n",
    "eta_Z, eta_F, adj = 0.95, 0.0, None\n",
    "fit_intercept = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_factors, loadings, ar_coef = trmf(\n",
    "    YY, n_components, n_order, C_Z, C_F, C_phi, eta_Z, eta_F, adj=None,\n",
    "    n_max_mf_iter=2, f_step_kind=\"fgm\", verbose=True, tol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast the latent factors and paste them with the ones inferred from the train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ahead, n_horizon = len(test_targets), 24\n",
    "\n",
    "forecast_factors = trmf_forecast_factors(\n",
    "    n_ahead + n_horizon, ar_coef, train_factors)\n",
    "\n",
    "# paste the train estimates to the prehistory and the dynamic forecast\n",
    "factors = np.concatenate([train_factors, forecast_factors], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the predictions:\n",
    "$$ \n",
    "    \\hat{Y}_{t+h\\mid t}\n",
    "        = \\hat{X}_{t+h\\mid t} F\n",
    "    \\,,\\quad\n",
    "    \\hat{X}_{t+h\\mid t}\n",
    "        = \\sum_{k=1}^p \\mathop{\\text{diag}}\\bigl(\\hat{\\phi}_{\\cdot k}\\bigr) \\hat{X}_{t+h-k\\mid t}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = scl.inverse_transform(np.dot(factors, loadings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"cyka ryba pizdos!!!\"\"\"  # a wise man once said..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the dynamics of the latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_cols = 3\n",
    "n_rows = (n_components + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 1.5*n_rows),\n",
    "                         sharex=True, sharey=False)\n",
    "\n",
    "for j, ax in zip(range(n_components), axes.flat):\n",
    "    ax.plot(factors[:-(n_ahead + n_horizon), j], lw=2)\n",
    "    ax.plot(factors[:-n_horizon, j], zorder=-1)\n",
    "    ax.plot(factors[:, j], zorder=-2, alpha=0.5)\n",
    "\n",
    "for ax in axes.flat[n_components:]:\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    ax.plot([0, len(factors)], [y_min, y_max], c=\"k\", lw=2, alpha=.25)\n",
    "    ax.plot([0, len(factors)], [y_max, y_min], c=\"k\", lw=2, alpha=.25)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_predicted_target = predicted_target[:-n_ahead-n_horizon:]\n",
    "\n",
    "trmf_mse = mean_squared_error(train_targets, train_predicted_target)\n",
    "lastknown_mse = mean_squared_error(train_targets[1:], train_targets[:-1])\n",
    "\n",
    "\n",
    "print(f\"\"\"train >>>\\nTRMF: {trmf_mse}\\nRunning Last: {lastknown_mse}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_predicted_target = predicted_target[-n_ahead-n_horizon:-n_horizon]\n",
    "\n",
    "trmf_mse = mean_squared_error(test_targets, test_predicted_target)\n",
    "last_mse = mean_squared_error(test_targets, train_targets[[-1] * len(test_targets)])\n",
    "lastknown_mse = mean_squared_error(test_targets[1:], test_targets[:-1])\n",
    "\n",
    "\n",
    "print(f\"\"\"test >>>\\nTRMF: {trmf_mse}\\nLast train: {last_mse}\\nRunning Last: {lastknown_mse}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "n_rows = (n_targets + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 1.5 * n_rows),\n",
    "                         sharex=True, sharey=False)\n",
    "\n",
    "for j, ax in zip(range(n_targets), axes.flat):\n",
    "    ax.plot(targets[:, j], lw=2)\n",
    "    ax.plot(predicted_target[:, j], zorder=2)\n",
    "    ax.axvspan(0, len(train_targets) - 1, color=\"k\", zorder=-1, alpha=0.05)\n",
    "    ax.set_title(f\"\"\"target {j}\"\"\")\n",
    "\n",
    "for ax in axes.flat[n_targets:]:\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    ax.plot([0, len(predicted_target)], [y_min, y_max], c=\"k\", lw=2, alpha=.25)\n",
    "    ax.plot([0, len(predicted_target)], [y_max, y_min], c=\"k\", lw=2, alpha=.25)\n",
    "#     ax.set_frame_on(False) ; ax.set_xticks([]) ; ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact the model will be reestimated upon arrival of new data, so\n",
    "this validation strategy, where we compare dynamic forecasts with\n",
    "the actual data is incompatible with the usage scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying to a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the demand and relationship data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand = pd.read_csv(\"./data/demand_out_encode_v2.csv\", header=0, index_col=None)\n",
    "df_demand = df_demand.set_index([\"item_code\", \"rpd\", \"future_flag\"]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topoout = pd.read_csv(\"./data/topo_out_encode_v2.csv\", header=0, index_col=None)\n",
    "df_topoout = df_topoout.set_index([\"item_code\", \"rpd\", \"future_flag\", \"parent_item_code\"]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of facts about the dataset:\n",
    "\n",
    "1. in `demand` and in `topoout` the `quantity` increases as the `future_flag` **decreases from 3 to 0**\n",
    "\n",
    "2. in `topoout` the `parent_quantity` increases as the `future_flag` **decreases from 3 to 0**\n",
    "\n",
    "3. in `topoout` the `parent_quantity` is always **less than or equal to** `quantity`\n",
    "\n",
    "4. the `parent_item_code` in `topoout` is a decomposition: **summing `quantity` in `topoout` over the `parent_item_code`** recovers the respective `quantity` in `demand`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $q^f_t$ denote the (vector) total quantity to be delivered by the end of month $t$\n",
    "though orders made $f$ months in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_total = df_demand[\"quantity\"].unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we have $q^{f-1}_t \\geq q^f_t$ for all $f$ and $t$\n",
    "* in general $q^f_t = \\delta^f_t + \\delta^{f+1}_t + \\ldots$, where $\\delta^f_t\\geq 0$ is the extra quantity ordered to be delivered by $t$ at $t-f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `periods=-1` shifts the values so that value at `j` becomes the value@`j-1`\n",
    "demand_delta = demand_total - demand_total.shift(periods=-1, axis=1).fillna(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus $q^f_t$ is a forward-looking quantity, which is known by the end of month $t-f$.\n",
    "\n",
    "* It might be a good predictor of $q^{f-1}_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_qty_total = df_topoout[\"parent_quantity\"].unstack(\"future_flag\", fill_value=0)\n",
    "parent_qty_delta = parent_qty_total - parent_qty_total.shift(periods=-1, axis=1).fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_total = df_topoout[\"quantity\"].unstack(\"future_flag\", fill_value=0)\n",
    "qty_delta = qty_total - qty_total.shift(periods=-1, axis=1).fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (qty_delta - parent_qty_delta >= 0).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = demand_total[0].unstack(\"item_code\", fill_value=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"./dataset.csv\")\n",
    "\n",
    "# df = pd.read_csv(\"./dataset.csv\", header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's pick a small set of items to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(n=32, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_targets, test_targets = train_test_split(df.values, test_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "x_form = Pipeline([\n",
    "    (\"log1p\", FunctionTransformer(func=np.log1p, inverse_func=np.expm1, validate=False, check_inverse=False)),\n",
    "    (\"scale\", StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_demand_scaled = x_form.fit_transform(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_order, n_components = 3, 20\n",
    "\n",
    "C_Z, C_F, C_phi = 5e-1, 1e0, 1e-4\n",
    "eta_Z, eta_F, adj = 0.80, 0.0, None\n",
    "fit_intercept = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_order, n_components = 3, 20\n",
    "\n",
    "C_Z, C_F, C_phi = 1e-2, 1e1, 1e-2\n",
    "eta_Z, eta_F, adj = 0.95, 0.0, None\n",
    "fit_intercept = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_factors, loadings, ar_coef = trmf(\n",
    "    train_demand_scaled, n_components, n_order,\n",
    "    C_Z, C_F, C_phi, eta_Z, eta_F, adj,\n",
    "    n_max_mf_iter=50, f_step_kind=\"fgm\", verbose=True, tol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_roots = np.round(np.stack([np.roots(np.r_[1, -phi[::-1]]) for phi in ar_coef], axis=0), 2)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, projection=\"polar\")\n",
    "ax.scatter(np.angle(est_roots), np.abs(est_roots), s=50)  #, c=colors, s=area, cmap='hsv', alpha=0.75)\n",
    "ax.set_rlim(0, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_objective_components(\n",
    "    train_demand_scaled, train_factors, loadings, ar_coef,\n",
    "    C_Z, C_F, C_phi, eta_Z, eta_F, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ahead, n_horizon = len(test_targets), 12\n",
    "\n",
    "forecast_factors = trmf_forecast_factors(\n",
    "    n_ahead + n_horizon, ar_coef, train_factors)\n",
    "\n",
    "# paste the train estimates to the prehistory and the dynamic forecast\n",
    "factors = np.concatenate([train_factors, forecast_factors], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_demand = x_form.inverse_transform(np.dot(factors, loadings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_cols = 3\n",
    "n_rows = (n_components + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 1.5*n_rows),\n",
    "                         sharex=True, sharey=False)\n",
    "\n",
    "for j, ax in zip(range(n_components), axes.flat):\n",
    "    ax.plot(factors[:-(n_ahead + n_horizon), j], lw=2)\n",
    "    ax.plot(factors[:-n_horizon, j], zorder=-1)\n",
    "    ax.plot(factors[:, j], zorder=-2, alpha=0.5)\n",
    "#     ax.plot(factors[:-48, j], zorder=-1)\n",
    "#     ax.plot(factors[:, j], zorder=-2, alpha=0.5)\n",
    "\n",
    "for ax in axes.flat[n_components:]:\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    ax.plot([0, len(factors)], [y_min, y_max], c=\"k\", lw=2, alpha=.25)\n",
    "    ax.plot([0, len(factors)], [y_max, y_min], c=\"k\", lw=2, alpha=.25)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "train_predicted_target = predicted_demand[:-n_ahead-n_horizon]\n",
    "\n",
    "trmf_mse = mean_squared_error(train_targets, train_predicted_target)\n",
    "lastknown_mse = mean_squared_error(train_targets[1:], train_targets[:-1])\n",
    "\n",
    "\n",
    "print(f\"\"\"train >>>\\nTRMF: {trmf_mse}\\nRunning Last: {lastknown_mse}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_target = predicted_demand[-n_ahead-n_horizon:-n_horizon]\n",
    "\n",
    "trmf_mse = mean_squared_error(test_targets, test_predicted_target)\n",
    "last_mse = mean_squared_error(test_targets, train_targets[[-1] * len(test_targets)])\n",
    "lastknown_mse = mean_squared_error(test_targets[1:], test_targets[:-1])\n",
    "\n",
    "\n",
    "print(f\"\"\"test >>>\\nTRMF: {trmf_mse}\\nLast train: {last_mse}\\nRunning Last: {lastknown_mse}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_targets = df.values.shape[1]\n",
    "for j in range(min(n_targets, 12)):\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    ax = fig.add_subplot(111, title=f\"\"\"target {j}\"\"\")\n",
    "    ax.plot(df.values[:, j], lw=2)\n",
    "    ax.plot(predicted_demand[:, j], zorder=2)\n",
    "    ax.axvspan(0, len(train_targets) - 1, color=\"k\", zorder=-1, alpha=0.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert False, \"\"\"Do not run!\"\"\"\n",
    "\n",
    "n_targets = df.values.shape[1]\n",
    "n_cols = 4\n",
    "n_rows = (n_targets + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 1.5 * n_rows),\n",
    "                         sharex=True, sharey=False)\n",
    "\n",
    "for j, ax in zip(range(n_targets), axes.flat):\n",
    "    ax.plot(df.values[:, j], lw=2)\n",
    "    ax.plot(predicted_demand[:, j], zorder=2)\n",
    "    ax.axvspan(0, len(train_targets) - 1, color=\"k\", zorder=-1, alpha=0.05)\n",
    "    ax.set_title(f\"\"\"target {j}\"\"\")\n",
    "\n",
    "for ax in axes.flat[n_targets:]:\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    ax.plot([0, len(predicted_demand)], [y_min, y_max], c=\"k\", lw=2, alpha=.25)\n",
    "    ax.plot([0, len(predicted_demand)], [y_max, y_min], c=\"k\", lw=2, alpha=.25)\n",
    "#     ax.set_frame_on(False) ; ax.set_xticks([]) ; ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert False, \"\"\"STOP!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gramm-Schmidt ftw!\n",
    "$$\n",
    "    f_k = g_k - \\sum_{i=1}^{k-1} \\langle g_k, u_i \\rangle u_i\n",
    "    \\,,\\quad u_k = \\tfrac{f_k}{\\|f_k\\|_2}\n",
    "    \\,. $$\n",
    "$$\n",
    "    f_k = (I - U_{:k} U_{:k}^{\\mathrm{T}}) g_k\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = random_state.normal(size=(250, 25))\n",
    "\n",
    "U = np.zeros_like(G)\n",
    "for k in range(G.shape[1]):\n",
    "    f_k = G[:, k] - np.dot(U[:, :k], np.dot(U[:, :k].T, G[:, k]))\n",
    "    U[:, k] = f_k / np.linalg.norm(f_k, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(\"float64[:, ::1](float64[:, ::1], int64)\",\n",
    "         fastmath=True, cache=False, error_model=\"numpy\")\n",
    "def gramm_schmidt(G, start):\n",
    "    for k in range(start, min(*G.shape)):\n",
    "        alpha = np.dot(G[:, :k].T, G[:, k])\n",
    "        G[:, k] -= np.dot(G[:, :k], alpha)\n",
    "        G[:, k] /= np.linalg.norm(G[:, k], 2)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = random_state.normal(size=(250, 250))\n",
    "G = gramm_schmidt(G, 0)\n",
    "\n",
    "assert np.allclose(np.dot(G.T, G), np.eye(*G.shape))\n",
    "assert np.allclose(np.dot(G, G.T), np.eye(*G.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"\"\"STOP!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.median(abs((scl.inverse_transform(np.dot(X, F)) - df.values) / np.maximum(df.values, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(ar_resid(X, phi), ord=2, axis=0)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abs(np.stack([np.roots(np.r_[1, -phi[j]]) for j in range(n_components)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(F==0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(F.T, c=\"red\")\n",
    "plt.plot(loadings.T, c=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that\n",
    "\\begin{equation}\n",
    "\\tfrac{\\eta}2 \\|X\\|^2\n",
    "    = \\sum_{j=1}^d \\tfrac{\\eta}2 \\|X e_j\\|^2\n",
    "    = \\sum_{j=1}^d \\tfrac{\\eta}2 (X e_j)^{\\mathrm{T}} (X e_j)\n",
    "    \\,,\n",
    "\\end{equation}\n",
    "whence\n",
    "\\begin{align}\n",
    "\\tfrac12 \\sum_{j=1}^d \\lambda_j\n",
    "    \\sum_{t=p+1}^T \\bigl(\n",
    "        X_{tj} - \\sum_{k=1}^p \\phi_{jk} X_{t-k,j}\n",
    "    \\bigr)^2\n",
    "    + \\tfrac{\\eta}2 \\|X\\|^2\n",
    "    &= \n",
    "    \\tfrac12 \\sum_{j=1}^d \\lambda_j\n",
    "        e_j^{\\mathrm{T}} X^{\\mathrm{T}} (A(\\phi_j) + \\eta I) X e_j\n",
    "    \\,,\n",
    "\\end{align}\n",
    "where $A(\\phi_j)$ is the AR-regularizer matrix computed for $\\beta = \\phi_j = e_j^{\\mathrm{T}} \\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective in the $X$-step is\n",
    "\\begin{align}\n",
    "Q(X \\mid F, \\phi)\n",
    "    &= \\tfrac12 \\|Y - X F\\|^2\n",
    "    + \\tfrac12 \\sum_{j=1}^d \\lambda_j\n",
    "        \\sum_{t=p+1}^T \\bigl(\n",
    "            X_{tj} - \\sum_{k=1}^p \\phi_{jk} X_{t-k,j}\n",
    "        \\bigr)^2\n",
    "    + \\tfrac{\\eta}2 \\|X\\|^2\n",
    "    \\\\\n",
    "    &= \\tfrac12 \\|Y - X F\\|^2\n",
    "    + \\tfrac12 \\sum_{j=1}^d\n",
    "        e_j^{\\mathrm{T}} X^{\\mathrm{T}} (\\lambda_j A(\\phi_j) + \\eta I) X e_j\n",
    "    \\\\\n",
    "    &= \\tfrac12 \\Bigl\\|\n",
    "        \\underbrace{\n",
    "            \\Bigl(Y - \\sum_{k\\neq j} X e_k e_k^{\\mathrm{T}} F\\Bigr)\n",
    "        }_{R_{-j}} - X e_j e_j^{\\mathrm{T}} F \\Bigr\\|^2\n",
    "    + \\tfrac12 \\sum_{j=1}^d\n",
    "        e_j^{\\mathrm{T}} X^{\\mathrm{T}} (\\lambda_j A(\\phi_j) + \\eta I) X e_j\n",
    "    \\\\\n",
    "    &= \\tfrac12 \\bigl\\| R_{-j} - z_j F_j^{\\mathrm{T}} \\bigr\\|^2\n",
    "    + \\tfrac12 z_j^{\\mathrm{T}} (\\lambda_j A(\\phi_j) + \\eta I) z_j\n",
    "    + \\tfrac12 \\sum_{k\\neq j}\n",
    "        e_k^{\\mathrm{T}} X^{\\mathrm{T}} (\\lambda_k A(\\phi_k) + \\eta I) X e_k\n",
    "      \\,,\n",
    "\\end{align}\n",
    "where $z_j = X e_j \\in \\mathbb{R}^{T\\times 1}$ and $F_j = e_j^{\\mathrm{T}} F \\in  \\mathbb{R}^{n \\times 1}$.\n",
    "\n",
    "Notice, that the $j$-th factor is the result of a regularized rank-1 approximation of $R_{-j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the optimization subproblem for the $j$-th factor conditional on\n",
    "$(F, \\phi)$ and all other factos (through the residual $R_{-j}$) is\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    & \\underset{z \\,\\mid\\, F, \\phi}{\\text{minimize}}\n",
    "      & & Q_j(z\\mid F, \\phi)\n",
    "          = \\tfrac12 \\bigl\\| R_{-j} - z F_j^{\\mathrm{T}} \\bigr\\|^2\n",
    "          + \\tfrac12 \\lambda_j\n",
    "              z^{\\mathrm{T}} (A(\\phi_j) + \\eta I) z\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the objective is\n",
    "\\begin{equation}\n",
    "\\nabla Q_j(z \\mid F, \\phi)\n",
    "    = - (R_{-j} - z F_j^{\\mathrm{T}}) F_j\n",
    "    + (\\lambda_j A(\\phi_j) + \\eta I) z\n",
    "    \\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the time series autoregressive embedding of order $p$ this problem\n",
    "becomes the familiar ridge regresssion:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    & \\underset{\\beta}{\\text{minimize}}\n",
    "      & & \\tfrac12 \\|y_{p+1:} - Z \\beta \\|^2\n",
    "          + \\tfrac{C}2 \\|\\beta \\|^2\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $Z_t = (y_{t-k})_{k=1}^p \\in \\mathbb{R}^{p\\times 1}$ and $Z \\in \\mathbb{R}^{t-p \\times p}$.\n",
    "The final $Z$ looks like $(y_{s:T-p+s})_{s=1}^p$ with $y_{s:t} = (y_\\tau)_{s \\leq \\tau < t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    & \\underset{X\\mid\\, F, \\phi}{\\text{minimize}}\n",
    "      & & \\tfrac12 \\|Y - X F\\|^2\n",
    "          + \\tfrac12 \\sum_{j=1}^d \\lambda_j\n",
    "              \\sum_{t=p+1}^T \\bigl(\n",
    "                  X_{tj} - \\sum_{k=1}^p \\phi_{jk} X_{t-k,j}\n",
    "              \\bigr)^2\n",
    "          + \\tfrac{\\eta}2 \\|X\\|^2\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $L^p_k$ be a rectangular matrix $T-p \\times T$ given by\n",
    "\\begin{equation}\n",
    "    L^p_k = \\begin{pmatrix}\n",
    "        \\underbrace{0}_{p-k} & \\underbrace{I}_{T-p} & \\underbrace{0}_{k}\n",
    "    \\end{pmatrix}\n",
    "    \\,.\n",
    "\\end{equation}\n",
    "Then for any $k$ we have $L^p_k Z = (Z_s)_{s=1+p-k}^{T-k}$, meaing that $L^p_k$\n",
    "represents the matrix instance of the lag operator.\n",
    "\n",
    "Hence,\n",
    "\\begin{equation}\n",
    "    \\bigl(X_{tj} - \\sum_{k=1}^p \\phi_{jk} X_{t-k,j}\\bigr)_{t=1+p}^T\n",
    "        = L^P_0 X e_j - \\sum_{k=1}^p L^P_k X e_j \\phi_{jk}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** The optimal $\\phi$ depends only on $X$. So, we may actually group the $X$- and $\\phi$ steps together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The joint step (rewrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we observe that conditional on $X$ the problem splits into independent subproblems:\n",
    "one w.r.t. $F$ and another w.r.t $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subproblems can be solved simultaneously, but by different methods.\n",
    "\n",
    "The $\\phi$-subproblem admits a closed form solution given by the ridge-regression estimate of $\\phi$.\n",
    "\n",
    "This is not the case in$F$-subrpoblem due to the nonnegativity constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIS no longer holds**\n",
    "Due to the graph regularizer, the targets are now tied together, so\n",
    "the following is for illustrative purposes only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, notice that unless $\\eta_F > 0$, the $F$-subproblem decomposes into $n$ simpler\n",
    "problems all of the form\n",
    "$$\n",
    "    \\min_{\\beta\\geq 0}\n",
    "        \\tfrac1{2 T n} \\| y - X \\beta\\|^2_2 + \\tfrac{\\lambda_F}{2 d n} \\|\\beta\\|^2\n",
    "\\,, $$\n",
    "for some time-series $y \\in \\mathbb{R}^{T\\times 1}$ and factors $X$. For instance, $y = Y e_j$\n",
    "and $\\beta = F e_j$ recover the $j$-th subproblem, which when stacked, result in\n",
    "the complete $F$-step problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The ADMM-type algorithm for the $F$-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FOC in subdifferential form are\n",
    "$$\n",
    "    % 0 \\in - X^{\\mathrm{T}} (y - X \\beta) + C \\beta + \\partial \\iota_{\\mathbb{R}^d_+}(\\beta)\n",
    "    X^{\\mathrm{T}}y \\in\n",
    "        \\bigl(X^{\\mathrm{T}} X + C I \\bigr) \\beta + \\partial \\iota_{\\mathbb{R}^d_+}(\\beta)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    L = \\tfrac12 \\| y - X \\beta\\|^2_2 + \\tfrac{\\lambda_F T}{2 d} \\|\\beta\\|^2\n",
    "    + \\iota_{\\mathbb{R}^d_+}(\\zeta)\n",
    "    % + \\tfrac1{2 \\nu}\\|\\beta - \\zeta\\|^2\n",
    "    % + \\tfrac1{\\nu} \\mu^{\\mathrm{T}} (\\beta - \\zeta)\n",
    "    + \\tfrac1{2 \\nu}\\|\\beta - \\zeta + \\mu\\|^2\n",
    "    - \\tfrac1{2 \\nu} \\|\\mu\\|^2\n",
    "    \\,. $$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $(X'X + \\tfrac{\\lambda_F T}{d} + \\tfrac1\\nu) \\beta_{t+1} = X'y + \\zeta_t - \\mu_t$\n",
    "\\begin{align}\n",
    "    \\beta_{t+1}\n",
    "        &= \\arg \\min_\\beta\n",
    "            \\tfrac12 \\| y - X \\beta\\|^2_2 + \\tfrac{\\lambda_F T}{2 d} \\|\\beta\\|^2\n",
    "            + \\tfrac1{2\\nu}\\|\\beta - (\\zeta_t - \\mu_t) \\|^2\n",
    "        \\,, \\\\\n",
    "    \\zeta_{t+1}\n",
    "        & = \\arg \\min_\\zeta\n",
    "            \\iota_{\\mathbb{R}^d_+}(\\zeta) + \\tfrac1{2\\nu}\\|\\zeta - (\\beta_{t+1} + \\mu_t)\\|^2\n",
    "        \\\\\n",
    "        &= {\\text{prox}}_{\\eta \\iota_{\\mathbb{R}^d_+}} (\\mu_t + \\beta_{t+1})\n",
    "            = (\\beta_{t+1} + \\mu_t)_+\n",
    "        \\,, \\\\\n",
    "     \\mu_{t+1}\n",
    "         &= \\mu_t + \\beta_{t+1} - \\zeta_{t+1}\n",
    "         \\,,\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FOC in subdifferential form are\n",
    "$$\n",
    "    x \\in z + \\nu \\partial \\iota_{\\mathbb{R}^d_+}(z)\n",
    "    \\Leftrightarrow\n",
    "    z = (\\mathrm{id} + \\nu \\partial \\iota_{\\mathbb{R}^d_+})^{-1}(x)\n",
    "    \\,. $$\n",
    "\n",
    "The subdifferential of $\\iota_{\\mathbb{R}^d_+}$ at $z$ is\n",
    "$$\n",
    "\\partial \\iota_{\\mathbb{R}^d_+}(z)\n",
    "    = \\{p\\colon 0 \\geq p'(y-z)\\, \\forall y\\geq 0 \\}\n",
    "    \\,. $$\n",
    "\n",
    "Solving all this is hard, so solving the optimization problem directly yields\n",
    "$$\n",
    "{\\text{prox}}_{\\eta \\iota_{\\mathbb{R}^d_+}}(x)\n",
    "    = (\\max\\{x_i, 0\\})_{i=1}^n\n",
    "\\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the relationship data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all `topoout['item_code'] -> topoout['parent_item_code']` pairs loaded with the associated quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = {}\n",
    "for (itm, rpd, f_f, par), (qty, par_qty) in tqdm.tqdm(df_topoout.iterrows()):\n",
    "    edges.setdefault((rpd, f_f), []).append((itm, par, {\"qty\": qty, \"par\": par_qty}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph all edges that were every encoutedet (**including the test period!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "master = nx.from_edgelist(((u, v) for lst in edges.values() for u, v, _ in lst), create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `index` to `vertex` bijection (`vertex` = `item_code`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itov = dict(enumerate(master))\n",
    "vtoi = dict(zip(itov.values(), itov.keys()))\n",
    "\n",
    "assert len(itov) == len(vtoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most functions in nx traverse the graph in the order of `G.nodes()`,\n",
    "which is equivalent to iterating over `G`:\n",
    "> If your node data is not needed, it is simpler and equivalent \n",
    "to use the expression ``for n in G``, or ``list(G)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2D geometric layout for the undirected version of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, gzip\n",
    "\n",
    "if os.path.exists(\"./master_pos.gz\"):\n",
    "    with gzip.open(\"./master_pos.gz\", \"r\") as fin:\n",
    "        master_pos = pickle.load(fin)\n",
    "else:\n",
    "    master_pos = nx.layout.kamada_kawai_layout(nx.Graph(master), scale=10)\n",
    "    with gzip.open(\"./master_pos.gz\", \"w\", compresslevel=9) as fout:\n",
    "        pickle.dump(master_pos, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a toposort on the master graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_stock = (\n",
    "    qty_delta - parent_qty_delta\n",
    ").sum(level=[\"item_code\", \"rpd\"])\n",
    "df_stock = accumulated_stock.sort_index().groupby([\"item_code\"]).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbunch = pd.unique(df_stock.index.get_level_values(\"item_code\"))\n",
    "items = list(nx.topological_sort(nx.subgraph(master, nbunch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitory, source, sink = [], [], []\n",
    "for item in items:\n",
    "    if not master.pred[item]:\n",
    "        source.append(item)\n",
    "    elif not master.succ[item]:\n",
    "        sink.append(item)\n",
    "    else:\n",
    "        transitory.append(item)\n",
    "    # end if\n",
    "# end for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = source + transitory + sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"\"\"STOP!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, floor, ceil\n",
    "\n",
    "n_plots, aspect = len(items), (4, 3)\n",
    "\n",
    "n_rows = round(sqrt(aspect[1] * float(n_plots) / aspect[0])+0.15)\n",
    "n_cols = round(sqrt(aspect[0] * float(n_plots) / aspect[1])+0.15)\n",
    "\n",
    "coef_w, coef_h = 3, 2\n",
    "\n",
    "figsize = n_cols * coef_w, n_rows * coef_h\n",
    "\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, squeeze=False,\n",
    "                         sharex=True, sharey=False,\n",
    "                         figsize=figsize, facecolor=\"white\", dpi=320)\n",
    "\n",
    "# reset all axes\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "# plot the datasets\n",
    "for item_code, ax in tqdm.tqdm(zip(items, axes.flat)):\n",
    "    ax.plot(df_stock.loc[item_code])\n",
    "    if not master.pred[item_code]:\n",
    "        fmt = \"item_code {}->\"\n",
    "    elif not master.succ[item_code]:\n",
    "        fmt = \"item_code ->{}\"\n",
    "    else:\n",
    "        fmt = \"item_code {}\"\n",
    "\n",
    "    ax.set_title(fmt.format(item_code))\n",
    "    ax.set_frame_on(True)\n",
    "\n",
    "# plot unused fillers\n",
    "for ax in axes.flat[n_plots:]:\n",
    "#     ax.plot([0, 1], [0, 1], color=\"black\", alpha=0.25, lw=2)\n",
    "#     ax.plot([1, 0], [0, 1], color=\"black\", alpha=0.25, lw=2)\n",
    "#     ax.set_xlim(0, 1)\n",
    "#     ax.set_ylim(0, 1)\n",
    "    pass\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"./all_toposort_stock.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of the master graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_demand = master.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the true demand data to each node in the `mater` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_demand = demand_delta.groupby(level=\"item_code\")\n",
    "\n",
    "values = {ic: df.loc[ic] for ic, df in ic_demand}\n",
    "nx.set_node_attributes(G_demand, values, \"delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a view into the node data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = G_demand.nodes(\"delta\", default=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create neighbour-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "for v in tqdm.tqdm(G_demand):\n",
    "    features.setdefault(v, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in tqdm.tqdm(G_demand):\n",
    "    upstream = {u: data[u] for u in G_demand.succ[v]}\n",
    "    # upstream.update({v: data[v].iloc[:0]})\n",
    "    if not upstream:\n",
    "        continue\n",
    "    df = pd.concat(upstream, axis=0)\n",
    "\n",
    "    features[v].update({\n",
    "        (\"upstream\", \"avg\"): df.mean(level=\"rpd\", axis=0),\n",
    "        (\"upstream\", \"max\"): df.max(level=\"rpd\", axis=0),\n",
    "        (\"upstream\", \"std\"): df.std(level=\"rpd\", axis=0).fillna(1.)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in tqdm.tqdm(G_demand):\n",
    "    downstream = {u: data[u] for u in G_demand.pred[v]}\n",
    "    # downstream.update({v: data[v].iloc[:0]})\n",
    "    if not downstream:\n",
    "        continue\n",
    "\n",
    "    df = pd.concat(downstream, axis=0) \n",
    "    features[v].update({\n",
    "        (\"downstream\", \"avg\"): df.mean(level=\"rpd\", axis=0),\n",
    "        (\"downstream\", \"max\"): df.max(level=\"rpd\", axis=0), \n",
    "        (\"downstream\", \"std\"): df.std(level=\"rpd\", axis=0).fillna(1.)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather features in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k: pd.concat(v, axis=1, names=[\"group\", \"feature\"])\n",
    "        for k, v in tqdm.tqdm(features.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach to graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G_demand, data, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"\"\"STOP!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything into a collection of graphs indexed by `(requested package date, future_flag)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = {key: nx.from_edgelist(lst, create_using=nx.DiGraph) for key, lst in edges.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the data geometry of the canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infalte(a, b, rtol=1e-1, atol=1e-2):\n",
    "    return a - np.maximum(abs(a), 1.) * rtol - atol, b + np.maximum(abs(b), 1.) * rtol + atol\n",
    "\n",
    "coord = np.stack(master_pos.values(), axis=0)\n",
    "c_min, c_max = _infalte(coord.min(axis=0), coord.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpd = 1\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111,\n",
    "                     xlim=(c_min[0], c_max[0]),\n",
    "                     ylim=(c_min[1], c_max[1]))\n",
    "\n",
    "nodes = set()\n",
    "for ff, col in zip([0, 1, 2, 3], [\"C0\", \"C1\", \"C2\", \"C3\"]):\n",
    "    G = graphs[rpd, ff]\n",
    "    nodes.update(G)\n",
    "    nx.draw_networkx_edges(G, pos=master_pos, ax=ax, edge_color=col,\n",
    "                           alpha=0.25, width=2.0, zorder=+12)\n",
    "\n",
    "xx, yy = zip(*[master_pos[v] for v in nodes])\n",
    "ax.scatter(xx, yy, s=5, color=\"blue\", zorder=-12, alpha=1.0)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_delta_skewed = pd.concat({\n",
    "    item_code: pd.concat([df.loc[item_code][s].shift(periods=-s) for s in [0, 1, 2, 3]], axis=1)\n",
    "    for item_code, df in demand_delta.groupby(\"item_code\")\n",
    "}, names=[\"item_code\"], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the data from all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dict(G_demand.nodes(\"features\")), names=[\"item_code\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average features for `ff=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.xs([\"avg\", 0], level=[\"feature\", \"future_flag\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset for autoregression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the `future_flag` at `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff0 = demand_delta[0].unstack(\"item_code\", fill_value=0)\n",
    "\n",
    "df_ff0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, although the `item_code` corresponds to the `vertex` in the master graph, the matrices\n",
    "produced by `networkx` are **indexed** by the **insertion order** of the vertices in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff0.rename(columns=vtoi).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A an all-in-one transformer for the dataframes indexed by `item_code`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "def df_right_matmul(data, matrix):\n",
    "    # item_code -> natual number + sort\n",
    "    df = data.rename(columns=vtoi).sort_index(axis=1)\n",
    "\n",
    "    # right-hand matmul\n",
    "    df = pd.DataFrame(safe_sparse_dot(df.values, matrix),\n",
    "                      index=df.index, columns=df.columns)\n",
    "\n",
    "    # natual number -> item_code + orignal item_code order\n",
    "    return df.rename(columns=itov).reindex(columns=data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symmetric normalized laplacian matrix is $L = D^{-\\tfrac12} \\mathcal{L} D^{-\\tfrac12}$:\n",
    "$$ \\mathcal{L}\n",
    "    = \\underbrace{\\text{diag}(\\delta_v)_{v\\in G}}_{D} - A\n",
    "    = \\biggl( e_v \\lvert G_v \\rvert - \\sum_{u\\in G_v} e_u \\biggr)_{v \\in G}\n",
    "\\,,\\quad \\mathcal{L} = \\mathcal{L}^{\\mathrm{T}} $$\n",
    "This does this $v\\mapsto q_v - \\sum_{u \\in G_v} \\tfrac1{\\sqrt{\\delta_u \\delta_v}} q_u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what we actually need is $L = \\mathcal{L} D^{-1}$ (since we will be left-applying the operator).\n",
    "It would do $v \\mapsto q_v - \\tfrac1{\\delta_v} \\sum_{u\\in G_v} q_u$, i.e. basically it would take\n",
    "the value at a node and subtract the average across its neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\bar{q}^{\\mathrm{T}} =\n",
    "    \\sum_{v\\in G} \\Bigl(\n",
    "        q_v - \\tfrac1{\\delta_v} \\sum_{u\\in G_v} q_u\n",
    "    \\Bigr) e_v^{\\mathrm{T}}\n",
    "    &= q^{\\mathrm{T}}\n",
    "        \\biggl(\n",
    "            \\sum_{v\\in G} \\Bigl(\n",
    "                e_v \\delta_v - \\sum_{u\\in G_v} e_u\n",
    "            \\Bigr) \\tfrac1{\\delta_v} e_v^{\\mathrm{T}}\n",
    "        \\biggr)\n",
    "    = q^{\\mathrm{T}}\n",
    "        \\sum_{p\\in G} \\biggl(\n",
    "            \\underbrace{\n",
    "                \\sum_{v\\in G} \\Bigl(\n",
    "                    e_v \\delta_v - \\sum_{u\\in G_v} e_u\n",
    "                \\Bigr) e_v^{\\mathrm{T}}\n",
    "            }_{D - A}\n",
    "        \\biggr) e_p \\tfrac1{\\delta_p} e_p^{\\mathrm{T}}\n",
    "    \\\\\n",
    "    &= q^{\\mathrm{T}} (D - A)\n",
    "        \\sum_{p\\in G} e_p \\tfrac1{\\delta_p} e_p^{\\mathrm{T}}\n",
    "    = q^{\\mathrm{T}} (D - A) D^{-1}\n",
    "\\,.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.normalized_laplacian_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = nx.adjacency_matrix(nx.Graph(master)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the adjacency of the undirected graph `master`\n",
    "adj = nx.adjacency_matrix(master).astype(float)\n",
    "degree = np.maximum(adj.sum(axis=0).A1, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $v\\mapsto G_v$ gives the set nodes that are endpoints of some edge from $v$.\n",
    "In the particular case of the `item_code - parent_item_code` relation this is the  set\n",
    "of parents of $v$ in $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix to average across all siblings is based on the following one:\n",
    "$$ A A^\\mathrm{T}\n",
    "    = \\biggl(\\sum_{u \\in G} \\sum_{v \\in G_u} e_u e_v^\\mathrm{T}\\biggr)\n",
    "        \\biggl(\\sum_{p \\in G} \\sum_{q \\in G_p} e_q e_p^\\mathrm{T}\\biggr)\n",
    "    = \\sum_{u, p \\in G} e_u \\Bigl(\n",
    "        \\sum_{v \\in G_u} \\sum_{q \\in G_p} e_v^\\mathrm{T} e_q\n",
    "    \\Bigr) e_p^\\mathrm{T}\n",
    "    = \\sum_{u, p \\in G} e_u\n",
    "        \\underbrace{\\lvert G_u \\cap G_p \\rvert}_{\\text{share the same parent}}\n",
    "    e_p^\\mathrm{T}\n",
    "    \\,. $$\n",
    "\n",
    "Siblings binary matrix is $S = \\mathtt{is-positive} (A A^\\mathrm{T}) - I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parents' binary matrix is $P = \\mathtt{is-positive} (A^\\mathrm{T} A) - I$, since\n",
    "$$ A^\\mathrm{T} A\n",
    "    = \\biggl(\\sum_{p \\in G} \\sum_{q \\in G_p} e_q e_p^\\mathrm{T}\\biggr)\n",
    "        \\biggl(\\sum_{u \\in G} \\sum_{v \\in G_u} e_u e_v^\\mathrm{T}\\biggr)\n",
    "    = \\sum_{q, v \\in G} e_q \\Bigl(\n",
    "        \\sum_{p\\colon q \\in G_p} \\sum_{u\\colon v \\in G_u} e_p^\\mathrm{T} e_u\n",
    "    \\Bigr) e_v^\\mathrm{T}\n",
    "    = \\sum_{q, v \\in G} e_q\n",
    "        \\underbrace{\\lvert x\\colon v, q \\in G_x \\rvert}_{\\text{have common child}}\n",
    "    e_v^\\mathrm{T}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAT = safe_sparse_dot(adj, adj.T, dense_output=True)\n",
    "ATA = safe_sparse_dot(adj.T, adj, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(AAT - np.diag(adj.sum(axis=1).A1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ATA - np.diag(adj.sum(axis=0).A1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "[itov[i] for i in adj_upstream[vtoi[9], :].indices]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the `upstream` adjacency matrix, which represents `child (rows) -> parent (columns)` relation.\n",
    "\n",
    "And compute the `upstream averaging` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_upstream = nx.adjacency_matrix(master)\n",
    "deg_upstream = adj_upstream.sum(axis=1).A1\n",
    "\n",
    "avg_upstream = adj_upstream.astype(float, copy=True)\n",
    "avg_upstream.data /= np.maximum(deg_upstream, 1.)[adj_upstream.indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `downstream` adjacency matrix, representin `parent (rows) -> child (columns)` relation, is just the transposed `upstream` matrix.\n",
    "\n",
    "And compute the `downstream averaging` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_downstream = adj_upstream.T.tocsr()\n",
    "deg_downstream = adj_downstream.sum(axis=1).A1\n",
    "\n",
    "avg_downstream = adj_downstream.astype(float, copy=True)\n",
    "avg_downstream.data /= np.maximum(deg_downstream, 1.)[adj_downstream.indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also compute the `upstream / downstream balancing` matrices to get the discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import eye\n",
    "\n",
    "unit_diag = eye(len(master))\n",
    "\n",
    "balance_ud = (adj_downstream + adj_upstream - 2 * unit_diag).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of neighbour values is given by $M = X L$, where $X$ is $\\times G$ matrix of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff0_downstream = df_right_matmul(df_ff0, avg_downstream)\n",
    "df_ff0_downstream.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff0_upstream = df_right_matmul(df_ff0, avg_upstream)\n",
    "df_ff0_upstream.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ff0_balance = df_right_matmul(df_ff0, balance_ud)\n",
    "df_ff0_balance.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_qty = df_demand[\"quantity\"].unstack(\"future_flag\", fill_value=0)\n",
    "# df_delta = df_qty - df_qty.shift(periods=-1, axis=1).fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "G = G_demand\n",
    "for v in G:\n",
    "    sib = {*chain(*(G.pred[u] for u in G.succ[v]))} - {v}\n",
    "    # sib = {*chain(*(product((u,), G.pred[u]) for u in G.succ[v]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_code = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax = fig.add_subplot(111, title=\"For item_code {}\".format(item_code))\n",
    "\n",
    "df_ff0[item_code].plot(ax=ax, label=\"level\")\n",
    "df_ff0_downstream[item_code].plot(ax=ax, label=\"downstream\")\n",
    "df_ff0_upstream[item_code].plot(ax=ax, label=\"upstream\")\n",
    "\n",
    "df_ff0_balance[item_code].plot(ax=ax, label=\"balance\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
